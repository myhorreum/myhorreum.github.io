03/10/20 (day 1) :

I came in 15 minutes early and met with Thomas, my supervisor and one of the people who interviewed me for the position. I was taken to the
office I would be doing my work in and introduced to the laptop (Dell) which I would be doing all work on. I was guided through the process of
setting up my uhtasi email specific and viber messaging accounts.

I was also introduced to my primary task which was to move many TASI web domains under the control of a web hosting panel. Thomas told me that 
the project should be done within a month. I was tasked with performing research on potential panels. I looked through paid panels such as cPanel
and Hosting Controller, as well as open source options such as Ajenti, VestaCP, and Webmin. I created a spreadsheet comparing and contrasting
various attributes the panels had with each other.

We ultimately chose to try out Webmin first. I also was interested in Hosting Controller, but had trouble finding info on how many domains were
allowed on its service as well as the cost of the service. I emailed them inquiring about it as well as planned to potentially test a demo of 
their service later on down the line (after working with Webmin).

03/11/20 (day 2) :

The night before day 2 I did further research on Webmin and found that Virtualmin is actually the web hosting panel service of Webmin. I reported
this info to Thomas and we decided to work with Virtualmin. Interestingly, the Virtualmin download also includes Webmin functionality (you can
toggle between Webmin and Virtualmin). 

Before any downloads, Thomas introduced me to TASI's vCenter. vCenter Server is the centralized management utility for VMware, and is used to manage
virtual machines, multiple ESXi hosts, and all dependent components from a single centralized location. The VMWare is used to create template Virual
Boxes for use. TASI uses Virtual Boxes to test software they have interest in before deciding to use it. In this case we created a Linux CentOS template. 
The template generates an IP address that can be accessed from other computers. I used this IP address to work in the box from my workspace laptop. 

Back on my laptop, I went to VMWare (had to disable the block on Flash in Chrome, also what will happen to VMWare once Chrome abandones Flash??) in
order to work with the Virtual Box environment. Note to log into VMWare I used a uhtasi account to gain access. I connected to the VMware remote
console which essentially connected my monitor to the server. My immediate goal was to get the networking going.

Since I was working on CentOS, I used the terminal command nmtui to set the IP address of our Virtual Box to a specific value and the netmask as
255.255.255.0. After changing this info, you must reset the network using the command systemctl restart network.service. 
To test if the box had internet I pinged 8.8.8.8 (Google).

Once the network was set up I used the IP address of the box to log in through PuTTY. I then changed the hostname and the root password to one of my choice 
using hostnamectl set-hostname your-new-hostname and sudo passwd root.

03/12/20 (day 3) :

My primary task for the day was to install Virtualmin/Webmin and test it out. Through PuTTY I tried to use wget to get the download script, however
wget was not available in PuTTY. To solve this, I used sudo yum install wget. However to complicate things further, yum was not working. I was
getting Yum Command Fails with “Another app is currently holding the yum lock”. I used Google to help solve this. It told me to use the ‘ps -ef’ command
to check for the PID being occupied by yum (it will be shown next to grep). All I had to do to fix this was use kill -9 13023 to stop the hold-up.

wget was finally working (note I did not go with curl because it was not getting me the script effectively). I then used sh install.sh to run the 
install script. Another problem did arise during the installation process. I was getting the message that my hostname server was not fully qualified.
I did research on FQDN (Fully Qualified Domain Name) and learned that I needed to update the /etc/hosts file. I had to put in the hostname and IP
of the server we had set up for Virtualmin use (the one we were currently logged in to PuTTY with). I updated the file with vi. I want to mention
that I exited vi without properly saving before editing how I wanted to; this auto-created a /etc/hosts.swp file. When using vi on the /etc/hosts
file, it gives the option of loading the /etc/hosts.swp file. To rid of this, simply use rm to delete the swap file. Also be sure your hostname 
matches the hostname in the file that you do vi on, or else it will not work.

Virtualmin was finally installed and we got to test it with a web address generated from our IP address. After experimenting with Virtualmin and 
Webmin, my manager and I were not making too much progress. We were confused on exactly what Virtualmin was trying to do. All we wanted was a place
to centrally manage existing web servers, while Virtualmin seemed to simplify new deployments of domains by auto creating a lot of stuff, 
overcomplicating a lot of what we were wanting to do. It seemed we might be getting more than we bargained for. However, at the end of the day,
before deciding to move on from Virtual/Webmin, we opted to do further research on them to see if it was still possible to do we wanted to do 
initially.

03/16/20 (day 4) :

Since Webmin/Virtualmin seemed to be too bulky for our needs, I recommended using ISPConfig. The installation process for ISPConfig was a bit more
lengthy, however I pushed through it much faster this time due to my experience installing Virtualmin. Towards the end of my work session, ISPConfig
was up and running and I added a test server to the domain listing. Thomas wants to see if Apache server integration is possible.

At home, I did further research on ISPConfig. I payed the $5 fee for the ISPConfig manual, and began studying it. It is apparent that 
"admin, clients, and resellers" are a big aspect of ISPConfig. These are three customizable user levels offered by ISPConfig. As of right now, it
does not seem like we will have use for client and reseller roles, though they do seem interesting. We should focus on admin because that gives full
control over the system. Currently I am studying 4.5.4 Domains section of the manual.

03/17/20 (day 5) :

I sent the ISPConfig manual to Thomas. He spent some time studying it before ultimately coming to the conclusion that it was very similar to Webmin.
Therefore, we deemed it best to abandon ISPConfig. Thomas told me to research Ansible, since Miguel had used it in the past. Ansible is a IT 
configuration and management tool used in organizing and even automating tasks.

At home, I looked into Ansible more. I found it had a GUI version called Ansible Tower created in response to the community wanting one. I would
present this info to Thomas tomorrow.

03/18/20 (day 6) :

I told Thomas about Ansible Tower and he believed it came with a price to use, so he told me to look at "regular" Ansible to see if I could find a
work-around, perhaps with plug-ins. I told him that Red Hat had taken over Ansible and that it does not seem to have a free option anymore. Finally
Thomas went to Miguel to figure out exactly what he wants in regards to web hosting control. It seemed that not even Miguel knew totally what he
wanted, he had no concrete idea in place. So, while Miguel figures that out, Thomas told me to move on to a new project: Windows Server Update Services.
Windows Server Update Services (WSUS) enables administrators to manage the distribution of updates and hotfixes released for Microsoft products to 
computers in a corporate environment.

I did research on this while waiting on Jose to configure the environment for me to work in.

03/19/20 (day 7) :

Today I was to configure and test out WSUS. I would do testing on a virtual server created in vCenter. I connected to this server through Remote 
Desktop Connection, which is like giving me access to another computer (in this case server). This server had WSUS installed by Jose. In WSUS, I 
was able to see the created group WSUSTest which had several computers from the TASI offices. Following an online guide, I checked for updates for
these computers and then updated them. I attempted to generate a report but could only generate the basic Computer one, not the detailed report option.
It seemed that WSUS had issues with timing out. I used Event Viewer and found the log of the connection failure. I Googled the Error: Connection Error,
Reset Server Node issue and found it to be a fairly common issue online. One solution would be to go into IIS (Internet Information Services) and
give WSUS access to unlimited memory. For some reason, the server was blank. I think Thomas then tried to move me to an admin role, however it was
still blank. I then searched IIS, right clicked it, and selected to access it as administrator, which then gave me the server. I finally gave WSUS
more memory.

However I still was getting the same issue with connection. Another guide told me to remove the WSUS Adminstration file under Sites in IIS. This did
not work either and actually seemed to break WSUS completely (the connection error was now permanent). I tried to reset the file (it was unlikely
deleted, so I would have to find and repoint it), but I had too much difficulty. I wanted to revert to a previous vCenter snapshot, but Thomas had
not taken one. Jose ended up reconfiguring it back to the original state and took a snapshot. I was now to rework with this again tomorrow.

03/20/20 (day 8) :

Along with giving WSUS unlimited memory in IIS, I also deleted the wsus file under %appdata%\Microsoft\MMC\. This seemed to work, however I was still
experiencing intermittent Error Connection incidents. I fixed this by going back to IIS and setting the Status of WsusPool to Stopped, waiting a bit
(you may get an error notification) and then setting it to Started again. This allowed me to view Updates and Approve them. Also when viewing Computers,
I am able to click on an individual computer and view its Detailed Computer Status Report. This shows the history record of Updates passed to this computer,
and shows the Approval, and the Status on whether it was Not Installed or Downloaded. This brought up the question as to when the Updates will be downloaded.
Are they put in a queue and done based on time? 

Also I want to mention that if you go to Updates and individually click an update, you can view its Detailed Update Status Report. I think what was causing
the crash was that I was previously trying to view them in bulk, which was overwhelming the connection and causing a time-out. 

Lastly, I looked into automating the update install process. I went to Options in the WSUS GUI and went into Automatic Approvals. I created a new rule
called WSUS Test Custom-Auto. I also noticed that within that rule I could customize update download deadlines. I set this to download "the next 
day after the approval at 3:00AM". I assume this will cause the download to be force installed at that time. Link with info :

https://docs.microsoft.com/de-de/security-updates/windowsupdateservices/18127631

However, unfortunately, when clicking the Run Rule, I get a Error Connection timeout after a bit of waiting. 

03/23/20 (day 9) :

I got in and tested the WSUS Automatic Approval that was not working yesterday. To my happiness, it worked without a problem. Thomas seemed happy with
my progress. He now wanted me to research on whether WSUS was capable of automating the Automatic Approval and report generations. Do I have to click
"Run Rule" each time, or is it possible to get it to do that on its own? Also can I create reports after each update cycle automatically, without me
having to do it physically? And can these reports be sent (possibly via email) without me having to do it myself? Surely there must be a way.

Also from now I was to be working remotely, from home, because of COVID-19. I am able to take my work laptop home. However Thomas had me install Dell
SonicWALL NetExtender. This would allow me to connect to uhtasi's VPN server from home, which runs continuously. I was to use Remote Desktop Connection
through this VPN connection.

03/25/20 (day 10) :

Working from home, I logged in via the methods described above. Now in WSUS, I could see in the WSUS Test computer group that the Last Status Report
had been updated since I had worked last time (for some workstations). I clicked on these workstations to view the report. To my dismay the workstation
was still stuck on where it last updated (Microsoft Silverlight). It is approved to Install but the status is that it is Not Installed.

How can I get past this?

I went back to the Automatic Approval Rule that I had set earlier. I noticed that it is only running for updates that fall in the Critical Updates and
Security Updates categories. The Microsoft Silverlight was categorized as Feature Packs. I changed the Auto Rule I created to encompass All Classifications,
not just the two specific categories I had prior. Maybe this will make a difference. I clicked Run Rule and I will check in a couple days.

However I was getting the Error Connection timeout. I restarted the WSUS Pool multiple times, but still was getting the timeout. I will come back to this
in the future, perhaps tomorrow or even back in Saunders itself.

Now I will move on to automating the email reports (having them sent via email automatically). I found a model PowerShell script online that I could use
for our use-case. 

https://www.experts-exchange.com/articles/27419/How-to-send-automatically-an-e-mail-with-a-report-of-computers-status-inside-WSUS-server.html

After modifying our needed credentials, I used Task Scheduler to run the script once a day to test it. However for troubleshooting purposes, I
began just executing the script by hand through its icon on the Desktop. To check if the email was actually being generated and sent to me, I went
to log in to my account back on the regular computer, however I was getting shut out from loggin. Thomas told me that this had to do with the VPN I
was going through. He rewired something and I could now get into my email. However the PowerShell email was not there. I had to do some reconfiguring.

Thomas sent me two scripts he had made himself in the past that had email functionality. Tomorrow I will look through them more indepth to see if I
can use them to help my own script.

03/27/20 (day 11) :

I got into the server and checked the Computer Status report to see if the Microsoft Silverlight had possibly changed at all. It hadn't, as I expected.
However the Status of the updates below Microsoft Silverlight (the ones that were previously Installed), have shifted to "Pending Reboot".

I moved on to try to run the Automatic Approval rule I could not run yesterday. Today it ran on the first time and gave the message that 777 updates
were approved. I will see if this gets the Microsoft Silverlight to finally change its Status to Installed.

I moved on to working with the PS Script. I took out the top smtp variables and focused on the bottom of the script (the Send Mail portion) and had
it match the one Thomas sent me. To my happiness, it worked and the email was in my mailbox. However, raw HTML was in the email which made it near
unreadable. I worked on fixing it. I realized that I had deleted the BodyAsHTML tag without knowing what it did. I put it back and the HTML was now
working as actual HTML and I could see the report in the table format as intended. I sent an email to Thomas as well and he was pleased.

Looking forward I want to see if I can possibly add a list of updates to the report. I also want to look to see what else PowerShell is capable of.
I could look into automation further, because I am still not certain if I need to click "Run Rule" manually every time. I also put the script into Task
Scheduler and set it to run weekly (Mondays at 12:59 PM).

03/30/20 (day 12) :

The emailed report was different from the first report, which showed that some updates went through. However when I went to check the generated reports
in WSUS, it was timing out and resetting the IIS pool did nothing. In the meantime I went to look at another way to use PowerShell scripting to 
obtain updates. It seems that I am transitioning more and more away from WSUS itself and more to PowerShell, since WSUS seems to have issues. I found
a whole slew of WSUS oriented cmdlets in Microsoft documentation:

https://docs.microsoft.com/en-us/powershell/module/updateservices

I looked more closely at Get-WsusUpdate. In PowerShell I ran the command quickly and all the updates on the system began printing onto the screen. I 
terminated the program because it was running on and on due to the thousands of updates shared between the workstations. I looked deeper and learned you
can use tags to only get updates based on attributes. Next work day I want to print updates that are Approved. Can I print updates that are both Approved
to Install and Installed or Approved to Install and Not Downloaded?

If I can get this going, I can eventually convert it into script form and put it into Task Scheduler to automate it. Then I would have two scripts running
based on WSUS reporting.

04/01/20 (day 13) :

Instead of using two separate scripts (the table-based report I got online and the list-of-updates I hoped to build myself), I realized it was probably
best to instead merge them into one. I wanted to figure out a way to possibly add the updates to the bottom of the report, underneath the table.

I received assistance from this guide :

https://devblogs.microsoft.com/scripting/get-windows-update-status-information-by-using-powershell/?irgwc=1&OCID=AID2000142_aff_7593_1375745&tduid=(ir__nkwgz0avcskfrkdtkk0sohziz22xnwu3rk6y206600)(7593)(1375745)()()&irclickid=_nkwgz0avcskfrkdtkk0sohziz22xnwu3rk6y206600

But ultimately could not get the script to work.

Also my NetExtender was not connecting. Thomas told me that NetExtender is notorious for this and told me how to remedy it. To resolve open Task Manager, 
go into Details tab, right click the NEGui.exe process and click End Process Tree, then go into the Services tab and right click the SONICWALL_NetExtender
service and click Start/Restart. If this does not work, try rebooting.

04/06/20 (day 14) :

I found another guide that had potential to be useful :

https://4sysops.com/archives/wsus-reporting-with-powershell/

I brought the script over and modified it to match my WSUS server. However, it would not run. Also, the errors in the PowerShell console were appearing
only for a split second before the console would close, which made it impossible for me to read the errors. I learned online that you can add a tag
at the end of the script for it to stay on the screen. I did this and could now read the errors. The error was coming from the wsus object that I had
created which was supposed to allow connection to the server. However it was saying the the remote connection could not be resolved. All the errors
afterwards were related to this, saying that the methods could not be called on a null-object (wsus). This was strange because the other script I had
been using used pretty much the exact code and was working. I asked Thomas and he led me through several troubleshooting methods but none worked. Tomorrow
I was to send him the script so he could try and figure it out himself.

04/07/20 (day 15) :

I showed Thomas the script and the methods he were going through were not solving the issue. He told me to also do all editing/troubleshooting in 
PowerShell ISE. This would eliminate the console closing suddently upon running the script and trying to see errors. Also it is superior to Notepad
because it highlights the syntax upon other things. 

Finally Thomas managed to identify the error. At the bottom of the script, where it ran the function, it could not identify the tag "wsus" for some
reason. I changed this to localhost as Thomas said and it now worked. However, after some time, the script timed-out. Specifically the GetUpdates
function operation timed-out individually for each call it was used. None of it was working. This is especially unfortunate because I was using 
PowerShell to get around the timing-out found with the WSUS software itself. So now we were back at square one.

To solve this, perhaps instead of looping through computers I could just try and work with one computer. However, upon thinking about this, it seems
the GetUpdates operation is actually being called individually upon each computer anyway, and is timing out (which is why the console has many timeout
errors after a certain amount of time - one timeout per computer). I think another way to approach solving this would be to clean out the WSUS update
database. There could be many updates that are outdated that are clogging up the database. Clearing this could solve the timeout issues. 

Online there are many cleanup scripts available. I tried one but could not get it to work. I ended up just using the built-in WSUS cleanup functionality.
However before the end of the day it did not finish its process, so I logged out without completing it. I will try again tomorrow.

04/09/20 (day 16) : 

I found another script online that looked promising. I downloaded it, but when I attempted to run it, it required a different execution policy. To
change this you have to go to PowerShell and run as administrator. Once you do this, you can use a command to change the Execution Policy for your
local computer. I changed this to Unrestricted which allows all scripts to run no matter what. You can also use a list command to view the policies
for each machine on the system.

However when I ran this script nothing was printing or happening, and I had no idea if it was working at all. I ended up going back to WSUS itself
and ran the cleaner wizard there. I began just running individual sections of updates instead of all at once to see if it made a difference. I
realized that it was freezing specifically on unneeded update files and unused update files. Looking at the stats on the other WSUS page, this makes
sense because there are over 100,000 updates in the database. It seems like all these updates are clogging the database, and when the computer tries
to clean/eliminate them, it just becomes overwhelmed.

I need to find a way to fix this database overpopulation.

04/14/20 (day 17) : 

I got the Adam script that is supposedly the best WSUS script around. After downloading it, I read that the only variable configuring needed to be
done were the email variables. I then went to run the script in Powershell admin ISE. I changed the Execution Policy to Bypass and ran the script
with the First Run tag. The script ran quickly, but had errors linked to an SQL related cmdlet not being installed. I then read in the script that 
you must have SQL Server Management Studio (SSMS) installed. I went online and downloaded it, but it would not install when double clicked because
it said that no program on the PC can open it. Online, I saw that a tutorial where the user first downloads the main SQL server, and then the SSMS.
I did this and then it seemed like I could now install the SSMS, but I was getting a lot of problems. I tried working with Task Manager to no success.
I ended up reverting to my last snapshot of the server. Thomas told me to see if the SSMS was not running because it was blocked. I went right-clicked
it, went into Properties, and saw that it did seem to be blocked. I un-blocked it and the installation was going through. However at the end, it said
that there is not enough disk space. I'm not sure how to fix this because I don't know what to delete on the server to free up space. 

04/15/20 (day 18) : 

To free some space I went to delete accumulated logs in C > Windows > Logs > CBS, as recommended by Thomas. You don't have to go through command line
to do this, you can just go through File Explorer. After deleting the logs, I went to install SSMS again. The install went through. However when I 
went to run the SSMS program, I was getting a weird obscure error that seemed to relate to missing configuration files. I did some research but was
confused on how it was not working or how to solve it. I happened to look at my disk space and realized that I had 0 bytes available to work with.
I was completely maxed out. This probably had something to do with the error. I realized that I would have to clear out more space in order to use
SSMS.

I downloaded and used WinDirStat. WinDirStat is a tool that shows disk-use storage by percentage. It is a useful tool to see what is taking up space
and where. After it ran, I was shocked to see that one file was taking up half of the entire disk space. The file was SUSDB.mdf and it was occupying
26/52 GB on the disk. I looked it up and learned that SUSDB.mdf was actually the WSUS database. It was incredibly bloated but I was glad to be able to
see it.

Thomas told me that he could extend the disk space, but then he asked if there were actually other drives on the system. I said yes, all the disks on
the system are: Local Disk (C), Data (E), and Directory (Z). Everything was stored on C, including all WSUS related files. Thomas told me that the E 
disk should actually be used to store WSUS updates. Thomas told me that it would be preferred for me to move WSUS to the E disk and have it update from
there permanently.

I did research and found a way to do this through Microsoft Documentation:
https://docs.microsoft.com/de-de/security-updates/windowsupdateservices/18127395

The command is wsusutil movecontent contentpath logfile, where contentpath and logfile are to be customized. I had trouble creating the path to the E
disk, the syntax of it was tripping me up. I realized the best way to do this was wsusutil movecontent E:\WSUS E:\WSUS\log.txt. I created a WSUS folder
within E. The command worked successfully and the command line told me that WSUS has been moved.

I began looking for the SUSDB.mdf file out of curiosity, to see if it was now in E. It was not. Neither could I find it in a File Explorer search. I
then used WinDirStat again and realized I had much more space now. But where was the SUSDB.mdf file? Tomorrow I will see if this impacts anything, and
I will proceed with SSMS and the clean-up script.

04/16/20 (day 19) : 

For some reason the C disk was still maxed out, and the SUSDB.mdf file was no where to be found. I chose to extend the disk. Thomas had to give more
available memory. To do this he had to merge all snapshots. Once it was done, I used this guide, which was pretty straightforward :

https://docs.microsoft.com/en-us/windows-server/storage/disk-management/extend-a-basic-volume

With the extra 10 GB of space on the C drive, I could now install SSMS without any problems. Once installed, I went to the PowerShell ISE to run the
script, it was not working for the sqlcmd. I then read through the script and learned that, at the bare mininum, I need these installed for the script:

SQL 2012/2014 - Version 11 - https://www.microsoft.com/en-us/download/details.aspx?id=36433
                      - ODBC Driver Version 11 - https://www.microsoft.com/en-gb/download/details.aspx?id=36434

However the ODBC Driver could not be installed on this operating system. For the Command Line Utilities 11 for SQL Server, it requires the ODBC Driver.

The other problem is that the SUSDB.mdf file should actually be moved to the new location. I used this guide :

https://www.tecknowledgebase.com/33/how-to-move-wsus-content-and-database-files-to-a-different-volume/

However the Servername: \\.\pipe\MSSQL$MICROSOFT##SSEE\sql\query was not connecting.

Lastly, I ran the WSUS software, but it would not load and this was due to a Database Error. I assume that the Database Error is from the missing
SUSDB.mdf file.

There is a lot of fixing up to do...

After thinking about things I thought of this :
1. Maybe the missing SUSDB.mdf file is because I don't have SQL Server downloaded? I only have SSMS downloaded.
2. Maybe the Servername: \\.\pipe\MSSQL$MICROSOFT##SSEE\sql\query is not connecting because of SQL Server not being downloaded?
3. Maybe the OBDC is because of the SQL Server not downloaded as well?

04/17/20 (day 20) : 

From reddit, a user told me that the Servername has been updated for newer OSes. I decided to go for his recommendation and it worked. This was
the new path :

\\.\pipe\MICROSOFT##WID\tsql\query

So I was in and I could see the directories including the SUSDB.mdf file that I could not find previously. I followed the guide I listed from the
previous day and got up to step 6 : Move the database files to the new drive or location where you are planning to keep it.

The reason why I was stuck was because I could not find the SUSDB.mdf outside of SSMS. Using File Explorer, nothing could be found. I tried looking
in Properties of the SUSDB.mdf file in SSMSS itself, but I was getting an error that would not allow me to see the Properties. Then, I looked online
and found a path that could potentially work, it was :

C > Windows > WID > Data

To my happiness, the path in fact did lead me to the SUSDB.mdf file.

Now, I had to move it from its current location in the C drive to the E drive where I want it to be. Initially, I had an error saying that I could 
not move the SUSDB.mdf file because because the file is open is another program. To fix this I had to kill the sqlservr.exe proccess. After this, I
could now move the files to the E drive.

However when I went back to SSMS, an error was coming up where the connecton to the database had been broken. Now nothing was working.

I think this had to do with me stopping the sqlservr.exe. In Task Manager, you can look at the Services tab, maybe it will be in here to get it to
start working again. Also I noticed WsusService was stopped as well. I may have to start this again further down the line. (Also don't forget that
I disabled the update services and the IIS). The update services could be the same as the WsusService.

However I could not find a way to restart sqlservr.exe

04/20/20 (day 21) : 

Back in Task Manager, I was looking around to see if I could find any way to restart the sqlservr.exe. Curiously, I came across MSSQL$MICROSOFT##WID
in the Services tab. This stuck out to me because I knew the sqlservr.exe was located in the Windows\WID\Binn file path. Note that WID stands for 
Windows Internal Database. Also, it was stopped.

I restarted it and its Status was now Running. I went back to SSMS and I was no longer getting the database error, so I knew that I had fixed it.

Now, before attaching the new SUSDB.mdf in SSMS, I wanted to delete the orininal copy in C > Windows > WID > Data. This is because it would be pointless
to have two copies, and the C disk should not have this file anyway (which is why we are doing this whole process of moving the file). I told Thomas
to take a Snapshot of the server before deleting the files.

I went to attach the new SUSMD.mdf in the E drive, however I was getting an error. I looked up a guide online and changed the properties of the SUSMD.mdf
to allow Users full access rights. However when I went back to attach it, I got another error. If you scroll right, you can see what type of error it is 
in the Message column. It said that I cannot attach a database with the same name. This is because the SUSDB.mdf both are named the same in the C and E drives,
obviously. I went to remove the SUSDB.mdf reference in the C file structure in SSMS, however I got an error saying the file is not found. I assume this is
because I deleted it earlier. 

I need to figure out how to fix this. I told Thomas to revert back to before I deleted the SUSMD.mdf file.

04/21/20 (day 22) : 

After reverting back to the Snapshot-ted state, I was back to having to detach the old SUSMD database. Remember to be running SSMS as admin while doing
this. Once detached, I went to attach the new database in the E drive. When trying to attach I initially had a problem, the error message was saying
something about read-only permissions. To fix this, I went to the WSUS folder I created in E drive (I was outside the folder). I right-clicked the
folder and went to Properties > Security. I then give Users full permissions. I got an error message about the log.ldf file but did not look at it too
deeply.

Back at the SSMS, I tried to attach again, but it did not work. It said that the .ldf file was the source of the problem. I went back the to E drive
to get to the .ldf file. I right-clicked on the file individually and noticed its User permissions were not fully set, I fully set them and the attach
went smoothly.

Note that when you detach, the SUSDB.mdf disappears, but when you attach, it reappears.

I then told Thomas to take a Snapshot because I want to delete the old SUSDB.mdf on the C drive.

Once the Snapshot was completed, I deleted the two duplicate files. I then remembered to restart Windows Update and IIS. I restarted IIS through command
prompt. Also, in Task Manager, I noticed WsusService and WSusCertServer were Stopped, so I set them to Running.

However when I went to run WSUS, it would not load the workstations or updates. I did more research online and came across this guide, and while I don't
think it completely matches this use-case, it has additional steps which I think are worth working through:

https://docs.microsoft.com/en-us/windows-server/administration/windows-server-update-services/manage/wid-to-sql-migration

I stopped Update Services and IIS again (using PowerShell, since its easier) and went through the later portions of the guide. 

I stopped before the "Edit the registry to point WSUS to the SQL Server Instance" of the guide.

Also, I learned that SSMS has an internal activity monitor. I utilized it, and interestingly could see a jump in activity when I initiated WSUS.
I guess this means that they are interacting with one another, but the connection is still failing.

It may be best to revert to before the moving of the databases.

My work process that to get WSUS working is as follows:

Move database to E drive -> Download the Command Line Utilities & ODBC Driver for the sqlcmd in the PS script -> Run the Adam super-script -> With the cleaned database, schedule other scripts for reporting info on WSUS

However since E drive move might not work, I might just skip that step and potentially come back to it further down the line.

04/22/20 (day 23) : 

I found an online video that went through the similar process of detaching-attaching the database, with a few extra steps. I will follow it and 
see if it works:

https://www.youtube.com/watch?v=-UOl3PBe1K0

Thomas reverted back to the snapshot that resulted in the merging of all previous snapshots. However it seems that this is before the disk extension.

I followed the guide and seemed to have gotten the SQL Server 2012 installed.

Tomorrow I will start the SSMS and go through with the guide. Hopefully it works!

04/23/20 (day 24) : 

After much frustration fiddling with the installations, I just decided to revert back to the very beginning. I extended the disk through disk manager,
installed SSMS, then followed the video exactly from the 3:20 mark. I went to WSUS to check if it was finally working, and it was. It took a while to 
load, which discouraged me, but finally loaded the computers on the server.

04/24/20 (day 25) : 

The first thing I did was delete the SUSDB and log from the C drive and then run WSUS to see if it still would connect. It did, which proved that
the WSUS was in fact interacting with the SUSDB on the E drive.

The next thing I did was download the command line utilities for SQL so that I could use the sqlcmd in the script. I went for Microsoft Command
Line Utilities 14.0 for SQL Server, it downloaded, but when I went to install it, it failed. It failed because it says it needs Microsoft ODBC
Driver 11 for SQL Server installed prior.

I went to install the ODBC Driver 11, but it said that it could not be installed on my operating system (Windows Server 2012 R2). I noticed that
there were other ODBC drivers. I went for ODBC Driver 13, and saw in the System Requirements -> Supported Operating System that Windows Server 2012
R2 was listed. So I went for it, but then I got an error saying the operating system was not supported.

I decided to delete the ODBC Driver 11 download file. I initially could not because it said it was open in a Windows Installer program. I went to
Task Manager and terminated it. I then deleted the file completely. Now that the ODBC Driver 13 file was isolated, I went through the install process
again and it worked. I now had it downloaded.

I then went through the install of the Command Line Utilities and it worked.

I went to run the script, but I got the same sqlcmd error. I then thought that maybe I had to restart the PowerShell ISE to get the script to recognize
the sqlcmd utility, and it did.

I then ran the script and it began working. It is now in the process of working through the script. I guess now we wait.

04/28/20 (day 26) : 

As I logged in to the Remote Server, I immediately noticed PowerShell ISE was not open, as well as a new .txt file being present on the Desktop.
PowerShell then auto opened, giving a message that the program was not shut down properly. This gave me the thought that maybe the script had been
interrupted. However when I opened the new .txt file, and read through it, it seemed like the script completed. 

The log gave information on the deleted updates as well as a bunch of other information, some of which I do not completely understand. But at the
end of the log it said that it took 01:02:18:50 to complete (which I interpreted as 1 Day 2 Hours 18 Minutes 50 Seconds). Also the script deleted
and compressed updates. I'm not totally sure the difference between deleting and compressing.

I was still a bit skeptical that the script completed because I swore that I had checked on the script at a time after that length. I could be wrong 
though.

I went to email the log to Thomas when I noticed in my email that there were several emails from the script. I then realized that it is an email for each
workstation computer connected to the WSUS server. I forwarded Thomas the largest log sent over email.

I then went and opened WSUS, ran some reports, and noticed it was displaying the information much faster, without a timeout so far, which is good.

I then decided to try and run the list-updates script I had downloaded previously since the database was now cleared up. It no longer timed out, but
instead I was getting a 

Method invocation failed because [System.Management.Automation.PSObject] doesn’t contain a method named ‘op_Addition'

on the reports variable within the script. I did research and found it was an easy fix. I just needed to add 

$reports = @()

to the script. The script now seemed to work. It ran, there was no output to the screen, and then it ended. It also was supposed to generate a .csv 
file, which I think it did. On the script it says that it will generate a .csv file on the Desktop called rep_wsus. This file was on the Desktop,
however when I clicked to open it, it was blank.

04/29/20 (day 27) : 

I moved on from the list-updates script and then just ran the report for the workstations through WSUS itself. I then exported the reports as Excel
and .pdf files. The Excel file was too big to email, so I just emailed the .pdf files to Thomas. It is worth noting that the reports had a mass of
Updates with "Non-Applicable" Status. These updates are approved to install but non-applicable to the workstation, therefore they are just sitting
in a sort of limbo state and not really doing anything.

I also looked it up on Microsoft Documentation:

"When referring to the status of one computer, Installed/Not Applicable means the update is not applicable to or required by that computer. 
When referring to the status for a computer group, the Installed/Not Applicable column displays the number of computers in the group for 
which the update is not applicable or not required."

Perhaps we should look into removing these updates.

The non-applicable could be related to the Auto Approval rule.

04/30/20 (day 28) : 

The goal is to clear out all of the Updates with the combination of Approval:Install and Status:NonApplicable. To do this you can use a PS script.
I created a quick script that uses Get-WSUSUpdate and Deny-WSUSUpdate. It should work but I was running into an error. The error could be something
very simple (like connecting to WSUS) however I am too inexperienced to solve it. I am waiting on help from reddit.

Also it is possible to decline updates with specific keywords like "office" "outlook" "powerpoint" etc. It could be useful down the line, if we 
know that certain workstations don't use those programs. We can design scripts tailored to specific workstations.

I showed Thomas the error and got the solution immediately. The error was that I was using InstalledNotApplicable when I should have been using
InstalledOrNotApplicable. I was not using the Or version because I was following the Microsoft Documentation and they did not have the Or.

https://docs.microsoft.com/en-us/powershell/module/updateservices/get-wsusupdate?view=win10-ps

They need to fix that. However, the solution was given in the console output. I should have payed closer attention:

Get-WsusUpdate : Cannot bind parameter 'Status'. Cannot convert value "InstalledNotApplicable" to type 
"Microsoft.UpdateServices.Commands.WsusUpdateInstallationState". Error: "Unable to match the identifier name 
InstalledNotApplicable to a valid enumerator name.  Specify one of the following enumerator names and try again: NoStatus, 
InstalledOrNotApplicable, InstalledOrNotApplicableOrNoStatus, Failed, Needed, FailedOrNeeded, Any"
At C:\Users\roman\Desktop\Clean-Non-Applicables.ps1:1 char:44
+ Get-WSUSUpdate -Classification All -Status InstalledNotApplicable  -Approval App ...
+                                            ~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : InvalidArgument: (:) [Get-WsusUpdate], ParameterBindingException
    + FullyQualifiedErrorId : CannotConvertArgumentNoMessage,Microsoft.UpdateServices.Commands.GetWsusUpdateComm

Now the script is running and it is taking awhile. Hopefully it is clearing out all the Non-Applicables.

The script finished after about an hour. I went into WSUS to check the Update Reports. I was surprised to see that the only updates left were those
with

Approval: Not approved
Status: Not Applicable

It looks like my script removed the updates that I had not intended to remove (I wanted the combo of [Approved + Not Applicable]). It removed the combo of 
[Install + Installed], basically all others except combo of [Not Approved + Not Applicable].
I need to change my script to fix this. I told Thomas to revert to the snapshot before today. Also it is worth noting that the updates went from around 
33000 down to 1670.

I was thinking... why is the option InstalledOrNotApplicable, not split into two separate options? Perhaps once the update is Approved and then Installed,
it is just as un-useful as Approved and then deemed Not Applicable (which would explain why they are fused). So then I assume Deny-WSUSUpdate is fine to use
on both Approved/Installed and Approved/Not-Applicable. I am not 100% sure though.

So the script did actually work, it denied all updates that were approved to install and had either been installed or deemed non-applicable. Now I just
need to confirm if it is fine to also deny updates that are already installed. My gut feeling is that it is.

05/04/20 (day 29) : 

The server was reverted to the time after the Adam script ran, actually to the moment I was fiddling with the 

Method invocation failed because [System.Management.Automation.PSObject] doesn’t contain a method named ‘op_Addition'

in the list-updates script (I remember telling Thomas to do a snapshot before I went in to fix this issue.).

Anyway, I recreated the Deny-NotApplicables script. Only this time I did the combo of [-Status InstalledOrNotApplicable + -Approval Any]
This is because I want to remove any NotApplicable update, no matter what its approval is (which is why the previous removal left the combo
of [Not Approved + Not Applicable]). I let the script run.

When it was finished I was surprised to see 32 updates left over, all with [Not Installed + Not Applicable]. Why did the script not
clear these Not Applicable updates out? I am a bit confused. Also I want to make sure the removed updates actually are on the workstations,
and not removed completely. I need to make sure the correct updates are getting through.

05/05/20 (day 30) : 

I went to All Updates in WSUS and saw that it had 0 updates of 34192 total updates shown. The display settings were showing a filter of
Approval:Approved and Status:Any. I switched the filter to Approval:Declined and Status:Any and now 34151 of 34192 updates were displayed.
I think these declined updates are the result of the script I had ran (the short script I quickly wrote myself). It is interesting to look
at some of the updates and see that many of these updates are stretching as far back as 2001. Is there a way to delete these updates out of
WSUS database? It seems they just take up space. I also remember that the database originally had around 127,000 updates before running the
Adam script. Currently the individual workstations each have under 50 updates each (Last nights sync brought the uku.uhtasi.local workstation
to 41 updates, however these all have the Non-Applicable status). Back to the All Updates, I can see the newly synced updates by using a filter
of Approval:Approved and Status:Any.

I now understand the WSUS update process:
1. Synchronization brings new updates from Microsoft to WSUS
    - Our synchronization is automatic, checking at 8:54:00 PM daily
    - Our last synchronization was on 5/4/20 with 9 new updates
2. The user then approves the updates either manually or automatically
    - I am using the Automatic Approval option, with a rule I set.
    - Haven't looked at it in awhile, I noticed it was set to only approve Critical Updates and Security Updates
    - I now changed it to approve All Classifications, because I noticed in the last synchronization there were Drivers 
    - It probably is just good to approve all updates from now on.
3. With the bulk of updates, we can deny the updates we don't need.
    - Using PowerShell scripts, we can sort out the Not-Applicable updates that make it through the Auto Approval Rule filter
    - We can also use PowerShell scripts to deny updates based on keywords like "outlook", "office", "powerpoint", etc
    - We can tailor make denial scripts for the needs of specific workstations.

05/07/20 (day 31) : 

Today I went into the office to check if the updates were actually getting installed on the workstations. The only one that had
the status of Installed, the Skype update, had in fact been installed. 

I looked into the Auto Approval Rule and remembered you can also confirm updates based on product. I noticed every single option
for product was checked off, meaning we were confirming updates for products that we probably are not even using. Perhaps if I cut 
down on these products and restrict it only to the ones we use/need it could potentially clear out all the Not Applicables.

05/08/20 (day 32) : 

I looked into the product aspect of the approval rule again, but I do not know what we need and do not need. I notice some products
are for older operatings systems, or operating systems we aren't using, however some are products that I am not sure if we use or not.
For now I won't touch these products. I will once I learn what exactly we need and don't need.

Also around 300 updates got approved since yesterday, but only one got installed on certain workstations. The rest were Not Applicables.

Also in the future I can email status reports through WSUS, it could be something worthwhile.

What I want to cement is the synchronization, approval, script-cleaning, reporting process. Get that process in a steady weekly rhythm.

05/11/20 (day 33) : 

General research on WSUS.

05/14/20 (day 34) : 

Lets formalize the WSUS scheduling:

1. Synchronize at 9:00:00 PM daily
2. Auto Approval is running constantly but is setting a deadline for [the next day after the approval at 3:00 AM]
    - The deadline will cause a forced restart at the time of the deadline if the update has not yet been installed
3. Set the reports to be emailed at Sunday 11:00PM.
    - I configured the emailing of WSUS update reports (per workstation) through Options -> E-Mail Notifications to send
      the reports to info@uhtasi.org
    - I will also send the tabluated report created with the PowerShell script.
4. The Clean-Not-Applicables script will be run on Monday at 12:00AM to remove all updates that have already been Installed
   or deemed Not-Applicable, to save space in the reporting.
5. The Adam script can be run if needed (if the database grows too bloated and decreases performance). Once we start noticing
   timeouts, this script should be run.

The one issue with this is that the E-Mail Notification configuration is that you can only set it to "Weekly". It does not let
you set the day (I want it Sunday). It does let you set a time field which I set to 11:00PM as I wanted in the formalization.

Thomas told me how to create my technical documentation:

Please see the attached documents for some examples of technical documentation. The main thing is getting all the documentation down in writing, we could always modify the format/style of the document later.

Do not distribute any of the documents I am sending you.

This is not specific to WSUS, but these are some technical documentation types that I think are useful:
1. User guides - For tasks that are nonobvious (or one-off types of activities).
2. An overview document - Describes the general methodology of your implementation.
3. Diagrams - If your implementation involves multiple moving parts (multiple VMs sending data to and from each other, etc). Some examples are data flow diagrams and infrastructure diagrams. Visio is what I use to make diagrams, let me know if you need a license.
4. Reference Documents - Things like cheat sheets for different programming languages (PowerShell, Bash, etc).
5. Presentations - To present the work you have done or to train other staff.

For WSUS I think the main types of technical documentation I'd like are an overview document, a presentation explaining your implementation and potentially a user guide.

05/15/20 (day 35) : 

Worked on the WSUS technical documentation.
Learned about Visio.
Will try and get TASI licensing next week.

05/18/20 (day 36) : 

Finished the WSUS technical documentation.

05/19/20 (day 37) : 

Decided to add Visio graphics to the WSUS technical documentation.

05/21/20 (day 38) : 

Created the PowerPoint presentation version of the WSUS technical documentation.

05/21/20 (day 39) : 

Research on ELK Stack.

05/22/20 (day 40) : 

Research on ELK Stack.

05/26/20 (day 41) : 

Research on ELK Stack.

05/27/20 (day 42) : 

Research on ELK Stack.

Summary of research:
ELK Stack represents multiple components (Elastisearch, Logstash, Kibana, Beats) that work together to aggregate pipelined data
and then visualize the data. Logstash is a server‑side data processing pipeline that ingests data from multiple sources 
simultaneously, transforms it, and then sends it to a "stash" like Elasticsearch. Kibana lets users visualize data with charts
and graphs in Elasticsearch. These components can all be installed on one main server.

The servers that want to participate with ELK stack should install Filebeat. Filebeat is a lightweight shipper for forwarding 
and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify,
collects log events, and forwards them either to Elasticsearch or Logstash for indexing.

Nginx (pronounced "engine-X") can also be installed onto the ELK stack server. Nginx is a reverse proxy. When the user connects to
the Nginx, it proxys the request to Kibana. Kibana pulls the data from Elasticsearch and the user gets the dashboard.

What is a proxy?
In computer networking, a proxy server is a server application or appliance that acts as an intermediary for requests from clients
seeking resources from servers that provide those resources.

The only ports exposed are the Logstash and the Nginx. The other internal ports are localhost.

05/28/20 (day 43) : 

Begin installation of ELK Stack. 

Thomas created a virtual server in https://vcsa.uhtasi.local/. When I initially tried to
get into the VCSA, it would not load after logging in (note this in FireFox). I switched to a Chrome browswer and managed to
get through without issues. When trying to load the console, it would not connect however (Connection Error). I then went to
the Summary tab and then the gear icon. I then went for the Web Console. It opened in a tab in the browser and was working fine.

I logged in using:
user : root
pass : C3nt0st3mpl4t3!

I wanted Thomas to take a snapshot before I fiddled with the networking in nmtui.

I was told to use this IP: 10.100.10.36

06/01/20 (day 44) : 

I got the networking working with nmtui. I began the installation process of the ELK stack using this guide:

https://blog.51sec.org/2019/10/easy-deploy-elk-into-centos-7.html

I got up to the step where I had to vi the /etc/yum.repos.d/elasticsearch.repo

Since I was in the Web Console, it seemed that I could not bring my mouse in to right-click the contents, instead I had to 
type it manually. I did so, but when I ran 

sudo yum install elasticsearch

I was getting an error. I vi'd back into the /etc/yum.repos.d/elasticsearch.repo and noticed the contents were reduced to 

put the contents here once u see them

Instead of trying to type the contents manually, I wished I could have just pasted them in. This is when I went to use PuTTy.
However I could not log in. I was getting Network Error: Connection refused. Thomas asked me if I could ping it. I pinged the
ELK server's IP from the ELK Web Console, and got a response. Obviously this would work. I then pinged the ELK server from the
Command Prompt outside the server, on the actual workstation, and then the ping did not work. This was because the ELK server's 
IP was not on the VPN access list. 

Next time I work I will disconnect and reconnect to the VPN and it should update.

06/02/20 (day 45) :

I began working through PuTTy and had a much easier time with the installation. I installed the netstat command. I also installed
Kibana and configured it so that I created an administrative Kibana user named kibanaadmin. The password for kibanaadmin is 
adminofkibana. I also created

sudo vi /etc/nginx/conf.d/elk.conf

This code configures Nginx to direct your server’s HTTP traffic to the Kibana application, which is listening on localhost:5601. 
Additionally, it configures Nginx to read the htpasswd.users file and require basic authentication. It is a Nginx server block file.

and added 

server_name 10.100.10.36;

which is the public ip for our ELK VM.

The Kibana visualization was live and could be viewed at 10.100.10.36/status

I then went through with the rest of the guide and installed logstash and filebeat.

This was the guide I used:

https://blog.51sec.org/2019/10/easy-deploy-elk-into-centos-7.html

This is the accompanying video:

https://www.youtube.com/watch?v=JeiPj8eSTSY&t=213s

Next time I work I want to see how to get clients integrated with the ELK server.

06/03/20 (day 46) :

We decided not to go with Filebeat on clients. Instead we will try and go with syslog. Initially we wanted to know whether the
format and facilities of syslog would need to be specific for smooth interaction with with the ELK stack.

From Reddit, a user gave me this response:

The facility doesn't matter (it gets recorded as a metadata field), the format is configurable. See here for fluentd, here for logstash, 
but the TL;DR is that it's treated as a log line. If you have RFC3164 logs then that'll work pretty much out of the box with the built in 
patterns. If you have a custom format you can still ship them as log lines, but if you want to parse the fields then you'll need to add a 
specific parser configuration. Both tools are pretty flexible in what they can do, so as long as your format is consistent I don't see any 
issues.

So we went with the old settings TASI had for their Graylog server: Enahnced syslog format and Messages Generated Internally by syslogd as the facility.

I told Thomas to set the port to 5044. We will see if this gets us logs from the clients into our ELK server.

06/04/20 (day 47) :

I could see Kibana receiving logs, about one every five seconds. However it turned out that these logs were filebeat logs from the
ELK server itself. I could also receive logs for certain events happening on the ELK server (like getting disconnected from PuTTY).
The problem was that it seemed I was not receiving logs from outside sources. Thomas tried sending packets from the firewall to my 
ELK server but no information was coming into Kibana. I then noticed that the port I gave him was incorrect, however it did not fix
the problem. I then noticed that my .yaml configuration files actually appeared to be commented out, which was weird.

Thomas said that it looked like I installed the ELK stack but did not configure it properly. 

06/05/20 (day 48) :

I wanted to do a completely new install from elastic.co because that is the official site. Thomas was skeptical however. I also told 
him how I had followed the guide I used exactly, but still could not receive outside data. I told Thomas that the install seemed to be
geared towards working with filebeat, not syslog. Thomas did some quick research on his own and realized that the syslog plug-in was
indeed needed, as I had said earlier. 

When I went to install the syslog plug-in, I noticed the command did not match my file directory structure. I did not have the same file
organization that the elastic.co had. The command was:

bin/logstash-plugin install logstash-output-syslog

However I did not have bin/logstash-plugin on my server. I looked around in my /etc/logstash folder, but could not find anything that seemed
similar to the bin/logstash-plugin. I decided to revert to before the installation and go with the elastic.co installation next time I work.


*
*
*
*
*
*
*
*
