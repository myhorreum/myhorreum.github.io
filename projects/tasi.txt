03/10/20 (day 1) :

I came in 15 minutes early and met with Thomas, my supervisor and one of the people who interviewed me for the position. I was taken to the
office I would be doing my work in and introduced to the laptop (Dell) which I would be doing all work on. I was guided through the process of
setting up my uhtasi email specific and viber messaging accounts.

I was also introduced to my primary task which was to move many TASI web domains under the control of a web hosting panel. Thomas told me that 
the project should be done within a month. I was tasked with performing research on potential panels. I looked through paid panels such as cPanel
and Hosting Controller, as well as open source options such as Ajenti, VestaCP, and Webmin. I created a spreadsheet comparing and contrasting
various attributes the panels had with each other.

We ultimately chose to try out Webmin first. I also was interested in Hosting Controller, but had trouble finding info on how many domains were
allowed on its service as well as the cost of the service. I emailed them inquiring about it as well as planned to potentially test a demo of 
their service later on down the line (after working with Webmin).

03/11/20 (day 2) :

The night before day 2 I did further research on Webmin and found that Virtualmin is actually the web hosting panel service of Webmin. I reported
this info to Thomas and we decided to work with Virtualmin. Interestingly, the Virtualmin download also includes Webmin functionality (you can
toggle between Webmin and Virtualmin). 

Before any downloads, Thomas introduced me to TASI's vCenter. vCenter Server is the centralized management utility for VMware, and is used to manage
virtual machines, multiple ESXi hosts, and all dependent components from a single centralized location. The VMWare is used to create template Virual
Boxes for use. TASI uses Virtual Boxes to test software they have interest in before deciding to use it. In this case we created a Linux CentOS template. 
The template generates an IP address that can be accessed from other computers. I used this IP address to work in the box from my workspace laptop. 

Back on my laptop, I went to VMWare (had to disable the block on Flash in Chrome, also what will happen to VMWare once Chrome abandones Flash??) in
order to work with the Virtual Box environment. Note to log into VMWare I used a uhtasi account to gain access. I connected to the VMware remote
console which essentially connected my monitor to the server. My immediate goal was to get the networking going.

Since I was working on CentOS, I used the terminal command nmtui to set the IP address of our Virtual Box to a specific value and the netmask as
255.255.255.0. After changing this info, you must reset the network using the command systemctl restart network.service. 
To test if the box had internet I pinged 8.8.8.8 (Google).

Once the network was set up I used the IP address of the box to log in through PuTTY. I then changed the hostname and the root password to one of my choice 
using hostnamectl set-hostname your-new-hostname and sudo passwd root.

03/12/20 (day 3) :

My primary task for the day was to install Virtualmin/Webmin and test it out. Through PuTTY I tried to use wget to get the download script, however
wget was not available in PuTTY. To solve this, I used sudo yum install wget. However to complicate things further, yum was not working. I was
getting Yum Command Fails with “Another app is currently holding the yum lock”. I used Google to help solve this. It told me to use the ‘ps -ef’ command
to check for the PID being occupied by yum (it will be shown next to grep). All I had to do to fix this was use kill -9 13023 to stop the hold-up.

wget was finally working (note I did not go with curl because it was not getting me the script effectively). I then used sh install.sh to run the 
install script. Another problem did arise during the installation process. I was getting the message that my hostname server was not fully qualified.
I did research on FQDN (Fully Qualified Domain Name) and learned that I needed to update the /etc/hosts file. I had to put in the hostname and IP
of the server we had set up for Virtualmin use (the one we were currently logged in to PuTTY with). I updated the file with vi. I want to mention
that I exited vi without properly saving before editing how I wanted to; this auto-created a /etc/hosts.swp file. When using vi on the /etc/hosts
file, it gives the option of loading the /etc/hosts.swp file. To rid of this, simply use rm to delete the swap file. Also be sure your hostname 
matches the hostname in the file that you do vi on, or else it will not work.

Virtualmin was finally installed and we got to test it with a web address generated from our IP address. After experimenting with Virtualmin and 
Webmin, my manager and I were not making too much progress. We were confused on exactly what Virtualmin was trying to do. All we wanted was a place
to centrally manage existing web servers, while Virtualmin seemed to simplify new deployments of domains by auto creating a lot of stuff, 
overcomplicating a lot of what we were wanting to do. It seemed we might be getting more than we bargained for. However, at the end of the day,
before deciding to move on from Virtual/Webmin, we opted to do further research on them to see if it was still possible to do we wanted to do 
initially.

03/16/20 (day 4) :

Since Webmin/Virtualmin seemed to be too bulky for our needs, I recommended using ISPConfig. The installation process for ISPConfig was a bit more
lengthy, however I pushed through it much faster this time due to my experience installing Virtualmin. Towards the end of my work session, ISPConfig
was up and running and I added a test server to the domain listing. Thomas wants to see if Apache server integration is possible.

At home, I did further research on ISPConfig. I payed the $5 fee for the ISPConfig manual, and began studying it. It is apparent that 
"admin, clients, and resellers" are a big aspect of ISPConfig. These are three customizable user levels offered by ISPConfig. As of right now, it
does not seem like we will have use for client and reseller roles, though they do seem interesting. We should focus on admin because that gives full
control over the system. Currently I am studying 4.5.4 Domains section of the manual.

03/17/20 (day 5) :

I sent the ISPConfig manual to Thomas. He spent some time studying it before ultimately coming to the conclusion that it was very similar to Webmin.
Therefore, we deemed it best to abandon ISPConfig. Thomas told me to research Ansible, since Miguel had used it in the past. Ansible is a IT 
configuration and management tool used in organizing and even automating tasks.

At home, I looked into Ansible more. I found it had a GUI version called Ansible Tower created in response to the community wanting one. I would
present this info to Thomas tomorrow.

03/18/20 (day 6) :

I told Thomas about Ansible Tower and he believed it came with a price to use, so he told me to look at "regular" Ansible to see if I could find a
work-around, perhaps with plug-ins. I told him that Red Hat had taken over Ansible and that it does not seem to have a free option anymore. Finally
Thomas went to Miguel to figure out exactly what he wants in regards to web hosting control. It seemed that not even Miguel knew totally what he
wanted, he had no concrete idea in place. So, while Miguel figures that out, Thomas told me to move on to a new project: Windows Server Update Services.
Windows Server Update Services (WSUS) enables administrators to manage the distribution of updates and hotfixes released for Microsoft products to 
computers in a corporate environment.

I did research on this while waiting on Jose to configure the environment for me to work in.

03/19/20 (day 7) :

Today I was to configure and test out WSUS. I would do testing on a virtual server created in vCenter. I connected to this server through Remote 
Desktop Connection, which is like giving me access to another computer (in this case server). This server had WSUS installed by Jose. In WSUS, I 
was able to see the created group WSUSTest which had several computers from the TASI offices. Following an online guide, I checked for updates for
these computers and then updated them. I attempted to generate a report but could only generate the basic Computer one, not the detailed report option.
It seemed that WSUS had issues with timing out. I used Event Viewer and found the log of the connection failure. I Googled the Error: Connection Error,
Reset Server Node issue and found it to be a fairly common issue online. One solution would be to go into IIS (Internet Information Services) and
give WSUS access to unlimited memory. For some reason, the server was blank. I think Thomas then tried to move me to an admin role, however it was
still blank. I then searched IIS, right clicked it, and selected to access it as administrator, which then gave me the server. I finally gave WSUS
more memory.

However I still was getting the same issue with connection. Another guide told me to remove the WSUS Adminstration file under Sites in IIS. This did
not work either and actually seemed to break WSUS completely (the connection error was now permanent). I tried to reset the file (it was unlikely
deleted, so I would have to find and repoint it), but I had too much difficulty. I wanted to revert to a previous vCenter snapshot, but Thomas had
not taken one. Jose ended up reconfiguring it back to the original state and took a snapshot. I was now to rework with this again tomorrow.

03/20/20 (day 8) :

Along with giving WSUS unlimited memory in IIS, I also deleted the wsus file under %appdata%\Microsoft\MMC\. This seemed to work, however I was still
experiencing intermittent Error Connection incidents. I fixed this by going back to IIS and setting the Status of WsusPool to Stopped, waiting a bit
(you may get an error notification) and then setting it to Started again. This allowed me to view Updates and Approve them. Also when viewing Computers,
I am able to click on an individual computer and view its Detailed Computer Status Report. This shows the history record of Updates passed to this computer,
and shows the Approval, and the Status on whether it was Not Installed or Downloaded. This brought up the question as to when the Updates will be downloaded.
Are they put in a queue and done based on time? 

Also I want to mention that if you go to Updates and individually click an update, you can view its Detailed Update Status Report. I think what was causing
the crash was that I was previously trying to view them in bulk, which was overwhelming the connection and causing a time-out. 

Lastly, I looked into automating the update install process. I went to Options in the WSUS GUI and went into Automatic Approvals. I created a new rule
called WSUS Test Custom-Auto. I also noticed that within that rule I could customize update download deadlines. I set this to download "the next 
day after the approval at 3:00AM". I assume this will cause the download to be force installed at that time. Link with info :

https://docs.microsoft.com/de-de/security-updates/windowsupdateservices/18127631

However, unfortunately, when clicking the Run Rule, I get a Error Connection timeout after a bit of waiting. 

03/23/20 (day 9) :

I got in and tested the WSUS Automatic Approval that was not working yesterday. To my happiness, it worked without a problem. Thomas seemed happy with
my progress. He now wanted me to research on whether WSUS was capable of automating the Automatic Approval and report generations. Do I have to click
"Run Rule" each time, or is it possible to get it to do that on its own? Also can I create reports after each update cycle automatically, without me
having to do it physically? And can these reports be sent (possibly via email) without me having to do it myself? Surely there must be a way.

Also from now I was to be working remotely, from home, because of COVID-19. I am able to take my work laptop home. However Thomas had me install Dell
SonicWALL NetExtender. This would allow me to connect to uhtasi's VPN server from home, which runs continuously. I was to use Remote Desktop Connection
through this VPN connection.

03/25/20 (day 10) :

Working from home, I logged in via the methods described above. Now in WSUS, I could see in the WSUS Test computer group that the Last Status Report
had been updated since I had worked last time (for some workstations). I clicked on these workstations to view the report. To my dismay the workstation
was still stuck on where it last updated (Microsoft Silverlight). It is approved to Install but the status is that it is Not Installed.

How can I get past this?

I went back to the Automatic Approval Rule that I had set earlier. I noticed that it is only running for updates that fall in the Critical Updates and
Security Updates categories. The Microsoft Silverlight was categorized as Feature Packs. I changed the Auto Rule I created to encompass All Classifications,
not just the two specific categories I had prior. Maybe this will make a difference. I clicked Run Rule and I will check in a couple days.

However I was getting the Error Connection timeout. I restarted the WSUS Pool multiple times, but still was getting the timeout. I will come back to this
in the future, perhaps tomorrow or even back in Saunders itself.

Now I will move on to automating the email reports (having them sent via email automatically). I found a model PowerShell script online that I could use
for our use-case. 

https://www.experts-exchange.com/articles/27419/How-to-send-automatically-an-e-mail-with-a-report-of-computers-status-inside-WSUS-server.html

After modifying our needed credentials, I used Task Scheduler to run the script once a day to test it. However for troubleshooting purposes, I
began just executing the script by hand through its icon on the Desktop. To check if the email was actually being generated and sent to me, I went
to log in to my account back on the regular computer, however I was getting shut out from loggin. Thomas told me that this had to do with the VPN I
was going through. He rewired something and I could now get into my email. However the PowerShell email was not there. I had to do some reconfiguring.

Thomas sent me two scripts he had made himself in the past that had email functionality. Tomorrow I will look through them more indepth to see if I
can use them to help my own script.

03/27/20 (day 11) :

I got into the server and checked the Computer Status report to see if the Microsoft Silverlight had possibly changed at all. It hadn't, as I expected.
However the Status of the updates below Microsoft Silverlight (the ones that were previously Installed), have shifted to "Pending Reboot".

I moved on to try to run the Automatic Approval rule I could not run yesterday. Today it ran on the first time and gave the message that 777 updates
were approved. I will see if this gets the Microsoft Silverlight to finally change its Status to Installed.

I moved on to working with the PS Script. I took out the top smtp variables and focused on the bottom of the script (the Send Mail portion) and had
it match the one Thomas sent me. To my happiness, it worked and the email was in my mailbox. However, raw HTML was in the email which made it near
unreadable. I worked on fixing it. I realized that I had deleted the BodyAsHTML tag without knowing what it did. I put it back and the HTML was now
working as actual HTML and I could see the report in the table format as intended. I sent an email to Thomas as well and he was pleased.

Looking forward I want to see if I can possibly add a list of updates to the report. I also want to look to see what else PowerShell is capable of.
I could look into automation further, because I am still not certain if I need to click "Run Rule" manually every time. I also put the script into Task
Scheduler and set it to run weekly (Mondays at 12:59 PM).

03/30/20 (day 12) :

The emailed report was different from the first report, which showed that some updates went through. However when I went to check the generated reports
in WSUS, it was timing out and resetting the IIS pool did nothing. In the meantime I went to look at another way to use PowerShell scripting to 
obtain updates. It seems that I am transitioning more and more away from WSUS itself and more to PowerShell, since WSUS seems to have issues. I found
a whole slew of WSUS oriented cmdlets in Microsoft documentation:

https://docs.microsoft.com/en-us/powershell/module/updateservices

I looked more closely at Get-WsusUpdate. In PowerShell I ran the command quickly and all the updates on the system began printing onto the screen. I 
terminated the program because it was running on and on due to the thousands of updates shared between the workstations. I looked deeper and learned you
can use tags to only get updates based on attributes. Next work day I want to print updates that are Approved. Can I print updates that are both Approved
to Install and Installed or Approved to Install and Not Downloaded?

If I can get this going, I can eventually convert it into script form and put it into Task Scheduler to automate it. Then I would have two scripts running
based on WSUS reporting.

04/01/20 (day 13) :

Instead of using two separate scripts (the table-based report I got online and the list-of-updates I hoped to build myself), I realized it was probably
best to instead merge them into one. I wanted to figure out a way to possibly add the updates to the bottom of the report, underneath the table.

I received assistance from this guide :

https://devblogs.microsoft.com/scripting/get-windows-update-status-information-by-using-powershell/?irgwc=1&OCID=AID2000142_aff_7593_1375745&tduid=(ir__nkwgz0avcskfrkdtkk0sohziz22xnwu3rk6y206600)(7593)(1375745)()()&irclickid=_nkwgz0avcskfrkdtkk0sohziz22xnwu3rk6y206600

But ultimately could not get the script to work.

Also my NetExtender was not connecting. Thomas told me that NetExtender is notorious for this and told me how to remedy it. To resolve open Task Manager, 
go into Details tab, right click the NEGui.exe process and click End Process Tree, then go into the Services tab and right click the SONICWALL_NetExtender
service and click Start/Restart. If this does not work, try rebooting.

04/06/20 (day 14) :

I found another guide that had potential to be useful :

https://4sysops.com/archives/wsus-reporting-with-powershell/

I brought the script over and modified it to match my WSUS server. However, it would not run. Also, the errors in the PowerShell console were appearing
only for a split second before the console would close, which made it impossible for me to read the errors. I learned online that you can add a tag
at the end of the script for it to stay on the screen. I did this and could now read the errors. The error was coming from the wsus object that I had
created which was supposed to allow connection to the server. However it was saying the the remote connection could not be resolved. All the errors
afterwards were related to this, saying that the methods could not be called on a null-object (wsus). This was strange because the other script I had
been using used pretty much the exact code and was working. I asked Thomas and he led me through several troubleshooting methods but none worked. Tomorrow
I was to send him the script so he could try and figure it out himself.

04/07/20 (day 15) :

I showed Thomas the script and the methods he were going through were not solving the issue. He told me to also do all editing/troubleshooting in 
PowerShell ISE. This would eliminate the console closing suddently upon running the script and trying to see errors. Also it is superior to Notepad
because it highlights the syntax upon other things. 

Finally Thomas managed to identify the error. At the bottom of the script, where it ran the function, it could not identify the tag "wsus" for some
reason. I changed this to localhost as Thomas said and it now worked. However, after some time, the script timed-out. Specifically the GetUpdates
function operation timed-out individually for each call it was used. None of it was working. This is especially unfortunate because I was using 
PowerShell to get around the timing-out found with the WSUS software itself. So now we were back at square one.

To solve this, perhaps instead of looping through computers I could just try and work with one computer. However, upon thinking about this, it seems
the GetUpdates operation is actually being called individually upon each computer anyway, and is timing out (which is why the console has many timeout
errors after a certain amount of time - one timeout per computer). I think another way to approach solving this would be to clean out the WSUS update
database. There could be many updates that are outdated that are clogging up the database. Clearing this could solve the timeout issues. 

Online there are many cleanup scripts available. I tried one but could not get it to work. I ended up just using the built-in WSUS cleanup functionality.
However before the end of the day it did not finish its process, so I logged out without completing it. I will try again tomorrow.

04/09/20 (day 16) : 

I found another script online that looked promising. I downloaded it, but when I attempted to run it, it required a different execution policy. To
change this you have to go to PowerShell and run as administrator. Once you do this, you can use a command to change the Execution Policy for your
local computer. I changed this to Unrestricted which allows all scripts to run no matter what. You can also use a list command to view the policies
for each machine on the system.

However when I ran this script nothing was printing or happening, and I had no idea if it was working at all. I ended up going back to WSUS itself
and ran the cleaner wizard there. I began just running individual sections of updates instead of all at once to see if it made a difference. I
realized that it was freezing specifically on unneeded update files and unused update files. Looking at the stats on the other WSUS page, this makes
sense because there are over 100,000 updates in the database. It seems like all these updates are clogging the database, and when the computer tries
to clean/eliminate them, it just becomes overwhelmed.

I need to find a way to fix this database overpopulation.

04/14/20 (day 17) : 

I got the Adam script that is supposedly the best WSUS script around. After downloading it, I read that the only variable configuring needed to be
done were the email variables. I then went to run the script in Powershell admin ISE. I changed the Execution Policy to Bypass and ran the script
with the First Run tag. The script ran quickly, but had errors linked to an SQL related cmdlet not being installed. I then read in the script that 
you must have SQL Server Management Studio (SSMS) installed. I went online and downloaded it, but it would not install when double clicked because
it said that no program on the PC can open it. Online, I saw that a tutorial where the user first downloads the main SQL server, and then the SSMS.
I did this and then it seemed like I could now install the SSMS, but I was getting a lot of problems. I tried working with Task Manager to no success.
I ended up reverting to my last snapshot of the server. Thomas told me to see if the SSMS was not running because it was blocked. I went right-clicked
it, went into Properties, and saw that it did seem to be blocked. I un-blocked it and the installation was going through. However at the end, it said
that there is not enough disk space. I'm not sure how to fix this because I don't know what to delete on the server to free up space. 

04/15/20 (day 18) : 

To free some space I went to delete accumulated logs in C > Windows > Logs > CBS, as recommended by Thomas. You don't have to go through command line
to do this, you can just go through File Explorer. After deleting the logs, I went to install SSMS again. The install went through. However when I 
went to run the SSMS program, I was getting a weird obscure error that seemed to relate to missing configuration files. I did some research but was
confused on how it was not working or how to solve it. I happened to look at my disk space and realized that I had 0 bytes available to work with.
I was completely maxed out. This probably had something to do with the error. I realized that I would have to clear out more space in order to use
SSMS.

I downloaded and used WinDirStat. WinDirStat is a tool that shows disk-use storage by percentage. It is a useful tool to see what is taking up space
and where. After it ran, I was shocked to see that one file was taking up half of the entire disk space. The file was SUSDB.mdf and it was occupying
26/52 GB on the disk. I looked it up and learned that SUSDB.mdf was actually the WSUS database. It was incredibly bloated but I was glad to be able to
see it.

Thomas told me that he could extend the disk space, but then he asked if there were actually other drives on the system. I said yes, all the disks on
the system are: Local Disk (C), Data (E), and Directory (Z). Everything was stored on C, including all WSUS related files. Thomas told me that the E 
disk should actually be used to store WSUS updates. Thomas told me that it would be preferred for me to move WSUS to the E disk and have it update from
there permanently.

I did research and found a way to do this through Microsoft Documentation:
https://docs.microsoft.com/de-de/security-updates/windowsupdateservices/18127395

The command is wsusutil movecontent contentpath logfile, where contentpath and logfile are to be customized. I had trouble creating the path to the E
disk, the syntax of it was tripping me up. I realized the best way to do this was wsusutil movecontent E:\WSUS E:\WSUS\log.txt. I created a WSUS folder
within E. The command worked successfully and the command line told me that WSUS has been moved.

I began looking for the SUSDB.mdf file out of curiosity, to see if it was now in E. It was not. Neither could I find it in a File Explorer search. I
then used WinDirStat again and realized I had much more space now. But where was the SUSDB.mdf file? Tomorrow I will see if this impacts anything, and
I will proceed with SSMS and the clean-up script.

04/16/20 (day 19) : 

For some reason the C disk was still maxed out, and the SUSDB.mdf file was no where to be found. I chose to extend the disk. Thomas had to give more
available memory. To do this he had to merge all snapshots. Once it was done, I used this guide, which was pretty straightforward :

https://docs.microsoft.com/en-us/windows-server/storage/disk-management/extend-a-basic-volume

With the extra 10 GB of space on the C drive, I could now install SSMS without any problems. Once installed, I went to the PowerShell ISE to run the
script, it was not working for the sqlcmd. I then read through the script and learned that, at the bare mininum, I need these installed for the script:

SQL 2012/2014 - Version 11 - https://www.microsoft.com/en-us/download/details.aspx?id=36433
                      - ODBC Driver Version 11 - https://www.microsoft.com/en-gb/download/details.aspx?id=36434

However the ODBC Driver could not be installed on this operating system. For the Command Line Utilities 11 for SQL Server, it requires the ODBC Driver.

The other problem is that the SUSDB.mdf file should actually be moved to the new location. I used this guide :

https://www.tecknowledgebase.com/33/how-to-move-wsus-content-and-database-files-to-a-different-volume/

However the Servername: \\.\pipe\MSSQL$MICROSOFT##SSEE\sql\query was not connecting.

Lastly, I ran the WSUS software, but it would not load and this was due to a Database Error. I assume that the Database Error is from the missing
SUSDB.mdf file.

There is a lot of fixing up to do...

After thinking about things I thought of this :
1. Maybe the missing SUSDB.mdf file is because I don't have SQL Server downloaded? I only have SSMS downloaded.
2. Maybe the Servername: \\.\pipe\MSSQL$MICROSOFT##SSEE\sql\query is not connecting because of SQL Server not being downloaded?
3. Maybe the OBDC is because of the SQL Server not downloaded as well?

04/17/20 (day 20) : 

From reddit, a user told me that the Servername has been updated for newer OSes. I decided to go for his recommendation and it worked. This was
the new path :

\\.\pipe\MICROSOFT##WID\tsql\query

So I was in and I could see the directories including the SUSDB.mdf file that I could not find previously. I followed the guide I listed from the
previous day and got up to step 6 : Move the database files to the new drive or location where you are planning to keep it.

The reason why I was stuck was because I could not find the SUSDB.mdf outside of SSMS. Using File Explorer, nothing could be found. I tried looking
in Properties of the SUSDB.mdf file in SSMSS itself, but I was getting an error that would not allow me to see the Properties. Then, I looked online
and found a path that could potentially work, it was :

C > Windows > WID > Data

To my happiness, the path in fact did lead me to the SUSDB.mdf file.

Now, I had to move it from its current location in the C drive to the E drive where I want it to be. Initially, I had an error saying that I could 
not move the SUSDB.mdf file because because the file is open is another program. To fix this I had to kill the sqlservr.exe proccess. After this, I
could now move the files to the E drive.

However when I went back to SSMS, an error was coming up where the connecton to the database had been broken. Now nothing was working.

I think this had to do with me stopping the sqlservr.exe. In Task Manager, you can look at the Services tab, maybe it will be in here to get it to
start working again. Also I noticed WsusService was stopped as well. I may have to start this again further down the line. (Also don't forget that
I disabled the update services and the IIS). The update services could be the same as the WsusService.

However I could not find a way to restart sqlservr.exe

04/20/20 (day 21) : 

Back in Task Manager, I was looking around to see if I could find any way to restart the sqlservr.exe. Curiously, I came across MSSQL$MICROSOFT##WID
in the Services tab. This stuck out to me because I knew the sqlservr.exe was located in the Windows\WID\Binn file path. Note that WID stands for 
Windows Internal Database. Also, it was stopped.

I restarted it and its Status was now Running. I went back to SSMS and I was no longer getting the database error, so I knew that I had fixed it.

Now, before attaching the new SUSDB.mdf in SSMS, I wanted to delete the orininal copy in C > Windows > WID > Data. This is because it would be pointless
to have two copies, and the C disk should not have this file anyway (which is why we are doing this whole process of moving the file). I told Thomas
to take a Snapshot of the server before deleting the files.

I went to attach the new SUSMD.mdf in the E drive, however I was getting an error. I looked up a guide online and changed the properties of the SUSMD.mdf
to allow Users full access rights. However when I went back to attach it, I got another error. If you scroll right, you can see what type of error it is 
in the Message column. It said that I cannot attach a database with the same name. This is because the SUSDB.mdf both are named the same in the C and E drives,
obviously. I went to remove the SUSDB.mdf reference in the C file structure in SSMS, however I got an error saying the file is not found. I assume this is
because I deleted it earlier. 

I need to figure out how to fix this. I told Thomas to revert back to before I deleted the SUSMD.mdf file.

04/21/20 (day 22) : 

After reverting back to the Snapshot-ted state, I was back to having to detach the old SUSMD database. Remember to be running SSMS as admin while doing
this. Once detached, I went to attach the new database in the E drive. When trying to attach I initially had a problem, the error message was saying
something about read-only permissions. To fix this, I went to the WSUS folder I created in E drive (I was outside the folder). I right-clicked the
folder and went to Properties > Security. I then give Users full permissions. I got an error message about the log.ldf file but did not look at it too
deeply.

Back at the SSMS, I tried to attach again, but it did not work. It said that the .ldf file was the source of the problem. I went back the to E drive
to get to the .ldf file. I right-clicked on the file individually and noticed its User permissions were not fully set, I fully set them and the attach
went smoothly.

Note that when you detach, the SUSDB.mdf disappears, but when you attach, it reappears.

I then told Thomas to take a Snapshot because I want to delete the old SUSDB.mdf on the C drive.

Once the Snapshot was completed, I deleted the two duplicate files. I then remembered to restart Windows Update and IIS. I restarted IIS through command
prompt. Also, in Task Manager, I noticed WsusService and WSusCertServer were Stopped, so I set them to Running.

However when I went to run WSUS, it would not load the workstations or updates. I did more research online and came across this guide, and while I don't
think it completely matches this use-case, it has additional steps which I think are worth working through:

https://docs.microsoft.com/en-us/windows-server/administration/windows-server-update-services/manage/wid-to-sql-migration

I stopped Update Services and IIS again (using PowerShell, since its easier) and went through the later portions of the guide. 

I stopped before the "Edit the registry to point WSUS to the SQL Server Instance" of the guide.

Also, I learned that SSMS has an internal activity monitor. I utilized it, and interestingly could see a jump in activity when I initiated WSUS.
I guess this means that they are interacting with one another, but the connection is still failing.

It may be best to revert to before the moving of the databases.

My work process that to get WSUS working is as follows:

Move database to E drive -> Download the Command Line Utilities & ODBC Driver for the sqlcmd in the PS script -> Run the Adam super-script -> With the cleaned database, schedule other scripts for reporting info on WSUS

However since E drive move might not work, I might just skip that step and potentially come back to it further down the line.

04/22/20 (day 23) : 

I found an online video that went through the similar process of detaching-attaching the database, with a few extra steps. I will follow it and 
see if it works:

https://www.youtube.com/watch?v=-UOl3PBe1K0

Thomas reverted back to the snapshot that resulted in the merging of all previous snapshots. However it seems that this is before the disk extension.

I followed the guide and seemed to have gotten the SQL Server 2012 installed.

Tomorrow I will start the SSMS and go through with the guide. Hopefully it works!

04/23/20 (day 24) : 

After much frustration fiddling with the installations, I just decided to revert back to the very beginning. I extended the disk through disk manager,
installed SSMS, then followed the video exactly from the 3:20 mark. I went to WSUS to check if it was finally working, and it was. It took a while to 
load, which discouraged me, but finally loaded the computers on the server.

04/24/20 (day 25) : 

The first thing I did was delete the SUSDB and log from the C drive and then run WSUS to see if it still would connect. It did, which proved that
the WSUS was in fact interacting with the SUSDB on the E drive.

The next thing I did was download the command line utilities for SQL so that I could use the sqlcmd in the script. I went for Microsoft Command
Line Utilities 14.0 for SQL Server, it downloaded, but when I went to install it, it failed. It failed because it says it needs Microsoft ODBC
Driver 11 for SQL Server installed prior.

I went to install the ODBC Driver 11, but it said that it could not be installed on my operating system (Windows Server 2012 R2). I noticed that
there were other ODBC drivers. I went for ODBC Driver 13, and saw in the System Requirements -> Supported Operating System that Windows Server 2012
R2 was listed. So I went for it, but then I got an error saying the operating system was not supported.

I decided to delete the ODBC Driver 11 download file. I initially could not because it said it was open in a Windows Installer program. I went to
Task Manager and terminated it. I then deleted the file completely. Now that the ODBC Driver 13 file was isolated, I went through the install process
again and it worked. I now had it downloaded.

I then went through the install of the Command Line Utilities and it worked.

I went to run the script, but I got the same sqlcmd error. I then thought that maybe I had to restart the PowerShell ISE to get the script to recognize
the sqlcmd utility, and it did.

I then ran the script and it began working. It is now in the process of working through the script. I guess now we wait.

04/28/20 (day 26) : 

As I logged in to the Remote Server, I immediately noticed PowerShell ISE was not open, as well as a new .txt file being present on the Desktop.
PowerShell then auto opened, giving a message that the program was not shut down properly. This gave me the thought that maybe the script had been
interrupted. However when I opened the new .txt file, and read through it, it seemed like the script completed. 

The log gave information on the deleted updates as well as a bunch of other information, some of which I do not completely understand. But at the
end of the log it said that it took 01:02:18:50 to complete (which I interpreted as 1 Day 2 Hours 18 Minutes 50 Seconds). Also the script deleted
and compressed updates. I'm not totally sure the difference between deleting and compressing.

I was still a bit skeptical that the script completed because I swore that I had checked on the script at a time after that length. I could be wrong 
though.

I went to email the log to Thomas when I noticed in my email that there were several emails from the script. I then realized that it is an email for each
workstation computer connected to the WSUS server. I forwarded Thomas the largest log sent over email.

I then went and opened WSUS, ran some reports, and noticed it was displaying the information much faster, without a timeout so far, which is good.

I then decided to try and run the list-updates script I had downloaded previously since the database was now cleared up. It no longer timed out, but
instead I was getting a 

Method invocation failed because [System.Management.Automation.PSObject] doesn’t contain a method named ‘op_Addition'

on the reports variable within the script. I did research and found it was an easy fix. I just needed to add 

$reports = @()

to the script. The script now seemed to work. It ran, there was no output to the screen, and then it ended. It also was supposed to generate a .csv 
file, which I think it did. On the script it says that it will generate a .csv file on the Desktop called rep_wsus. This file was on the Desktop,
however when I clicked to open it, it was blank.

04/29/20 (day 27) : 

I moved on from the list-updates script and then just ran the report for the workstations through WSUS itself. I then exported the reports as Excel
and .pdf files. The Excel file was too big to email, so I just emailed the .pdf files to Thomas. It is worth noting that the reports had a mass of
Updates with "Non-Applicable" Status. These updates are approved to install but non-applicable to the workstation, therefore they are just sitting
in a sort of limbo state and not really doing anything.

I also looked it up on Microsoft Documentation:

"When referring to the status of one computer, Installed/Not Applicable means the update is not applicable to or required by that computer. 
When referring to the status for a computer group, the Installed/Not Applicable column displays the number of computers in the group for 
which the update is not applicable or not required."

Perhaps we should look into removing these updates.

The non-applicable could be related to the Auto Approval rule.

04/30/20 (day 28) : 

The goal is to clear out all of the Updates with the combination of Approval:Install and Status:NonApplicable. To do this you can use a PS script.
I created a quick script that uses Get-WSUSUpdate and Deny-WSUSUpdate. It should work but I was running into an error. The error could be something
very simple (like connecting to WSUS) however I am too inexperienced to solve it. I am waiting on help from reddit.

Also it is possible to decline updates with specific keywords like "office" "outlook" "powerpoint" etc. It could be useful down the line, if we 
know that certain workstations don't use those programs. We can design scripts tailored to specific workstations.

I showed Thomas the error and got the solution immediately. The error was that I was using InstalledNotApplicable when I should have been using
InstalledOrNotApplicable. I was not using the Or version because I was following the Microsoft Documentation and they did not have the Or.

https://docs.microsoft.com/en-us/powershell/module/updateservices/get-wsusupdate?view=win10-ps

They need to fix that. However, the solution was given in the console output. I should have payed closer attention:

Get-WsusUpdate : Cannot bind parameter 'Status'. Cannot convert value "InstalledNotApplicable" to type 
"Microsoft.UpdateServices.Commands.WsusUpdateInstallationState". Error: "Unable to match the identifier name 
InstalledNotApplicable to a valid enumerator name.  Specify one of the following enumerator names and try again: NoStatus, 
InstalledOrNotApplicable, InstalledOrNotApplicableOrNoStatus, Failed, Needed, FailedOrNeeded, Any"
At C:\Users\roman\Desktop\Clean-Non-Applicables.ps1:1 char:44
+ Get-WSUSUpdate -Classification All -Status InstalledNotApplicable  -Approval App ...
+                                            ~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : InvalidArgument: (:) [Get-WsusUpdate], ParameterBindingException
    + FullyQualifiedErrorId : CannotConvertArgumentNoMessage,Microsoft.UpdateServices.Commands.GetWsusUpdateComm

Now the script is running and it is taking awhile. Hopefully it is clearing out all the Non-Applicables.

The script finished after about an hour. I went into WSUS to check the Update Reports. I was surprised to see that the only updates left were those
with

Approval: Not approved
Status: Not Applicable

It looks like my script removed the updates that I had not intended to remove (I wanted the combo of [Approved + Not Applicable]). It removed the combo of 
[Install + Installed], basically all others except combo of [Not Approved + Not Applicable].
I need to change my script to fix this. I told Thomas to revert to the snapshot before today. Also it is worth noting that the updates went from around 
33000 down to 1670.

I was thinking... why is the option InstalledOrNotApplicable, not split into two separate options? Perhaps once the update is Approved and then Installed,
it is just as un-useful as Approved and then deemed Not Applicable (which would explain why they are fused). So then I assume Deny-WSUSUpdate is fine to use
on both Approved/Installed and Approved/Not-Applicable. I am not 100% sure though.

So the script did actually work, it denied all updates that were approved to install and had either been installed or deemed non-applicable. Now I just
need to confirm if it is fine to also deny updates that are already installed. My gut feeling is that it is.

05/04/20 (day 29) : 

The server was reverted to the time after the Adam script ran, actually to the moment I was fiddling with the 

Method invocation failed because [System.Management.Automation.PSObject] doesn’t contain a method named ‘op_Addition'

in the list-updates script (I remember telling Thomas to do a snapshot before I went in to fix this issue.).

Anyway, I recreated the Deny-NotApplicables script. Only this time I did the combo of [-Status InstalledOrNotApplicable + -Approval Any]
This is because I want to remove any NotApplicable update, no matter what its approval is (which is why the previous removal left the combo
of [Not Approved + Not Applicable]). I let the script run.

When it was finished I was surprised to see 32 updates left over, all with [Not Installed + Not Applicable]. Why did the script not
clear these Not Applicable updates out? I am a bit confused. Also I want to make sure the removed updates actually are on the workstations,
and not removed completely. I need to make sure the correct updates are getting through.

05/05/20 (day 30) : 

I went to All Updates in WSUS and saw that it had 0 updates of 34192 total updates shown. The display settings were showing a filter of
Approval:Approved and Status:Any. I switched the filter to Approval:Declined and Status:Any and now 34151 of 34192 updates were displayed.
I think these declined updates are the result of the script I had ran (the short script I quickly wrote myself). It is interesting to look
at some of the updates and see that many of these updates are stretching as far back as 2001. Is there a way to delete these updates out of
WSUS database? It seems they just take up space. I also remember that the database originally had around 127,000 updates before running the
Adam script. Currently the individual workstations each have under 50 updates each (Last nights sync brought the uku.uhtasi.local workstation
to 41 updates, however these all have the Non-Applicable status). Back to the All Updates, I can see the newly synced updates by using a filter
of Approval:Approved and Status:Any.

I now understand the WSUS update process:
1. Synchronization brings new updates from Microsoft to WSUS
    - Our synchronization is automatic, checking at 8:54:00 PM daily
    - Our last synchronization was on 5/4/20 with 9 new updates
2. The user then approves the updates either manually or automatically
    - I am using the Automatic Approval option, with a rule I set.
    - Haven't looked at it in awhile, I noticed it was set to only approve Critical Updates and Security Updates
    - I now changed it to approve All Classifications, because I noticed in the last synchronization there were Drivers 
    - It probably is just good to approve all updates from now on.
3. With the bulk of updates, we can deny the updates we don't need.
    - Using PowerShell scripts, we can sort out the Not-Applicable updates that make it through the Auto Approval Rule filter
    - We can also use PowerShell scripts to deny updates based on keywords like "outlook", "office", "powerpoint", etc
    - We can tailor make denial scripts for the needs of specific workstations.

05/07/20 (day 31) : 

Today I went into the office to check if the updates were actually getting installed on the workstations. The only one that had
the status of Installed, the Skype update, had in fact been installed. 

I looked into the Auto Approval Rule and remembered you can also confirm updates based on product. I noticed every single option
for product was checked off, meaning we were confirming updates for products that we probably are not even using. Perhaps if I cut 
down on these products and restrict it only to the ones we use/need it could potentially clear out all the Not Applicables.

05/08/20 (day 32) : 

I looked into the product aspect of the approval rule again, but I do not know what we need and do not need. I notice some products
are for older operatings systems, or operating systems we aren't using, however some are products that I am not sure if we use or not.
For now I won't touch these products. I will once I learn what exactly we need and don't need.

Also around 300 updates got approved since yesterday, but only one got installed on certain workstations. The rest were Not Applicables.

Also in the future I can email status reports through WSUS, it could be something worthwhile.

What I want to cement is the synchronization, approval, script-cleaning, reporting process. Get that process in a steady weekly rhythm.

05/11/20 (day 33) : 

General research on WSUS.

05/14/20 (day 34) : 

Lets formalize the WSUS scheduling:

1. Synchronize at 9:00:00 PM daily
2. Auto Approval is running constantly but is setting a deadline for [the next day after the approval at 3:00 AM]
    - The deadline will cause a forced restart at the time of the deadline if the update has not yet been installed
3. Set the reports to be emailed at Sunday 11:00PM.
    - I configured the emailing of WSUS update reports (per workstation) through Options -> E-Mail Notifications to send
      the reports to info@uhtasi.org
    - I will also send the tabluated report created with the PowerShell script.
4. The Clean-Not-Applicables script will be run on Monday at 12:00AM to remove all updates that have already been Installed
   or deemed Not-Applicable, to save space in the reporting.
5. The Adam script can be run if needed (if the database grows too bloated and decreases performance). Once we start noticing
   timeouts, this script should be run.

The one issue with this is that the E-Mail Notification configuration is that you can only set it to "Weekly". It does not let
you set the day (I want it Sunday). It does let you set a time field which I set to 11:00PM as I wanted in the formalization.

Thomas told me how to create my technical documentation:

Please see the attached documents for some examples of technical documentation. The main thing is getting all the documentation down in writing, we could always modify the format/style of the document later.

Do not distribute any of the documents I am sending you.

This is not specific to WSUS, but these are some technical documentation types that I think are useful:
1. User guides - For tasks that are nonobvious (or one-off types of activities).
2. An overview document - Describes the general methodology of your implementation.
3. Diagrams - If your implementation involves multiple moving parts (multiple VMs sending data to and from each other, etc). Some examples are data flow diagrams and infrastructure diagrams. Visio is what I use to make diagrams, let me know if you need a license.
4. Reference Documents - Things like cheat sheets for different programming languages (PowerShell, Bash, etc).
5. Presentations - To present the work you have done or to train other staff.

For WSUS I think the main types of technical documentation I'd like are an overview document, a presentation explaining your implementation and potentially a user guide.

05/15/20 (day 35) : 

Worked on the WSUS technical documentation.
Learned about Visio.
Will try and get TASI licensing next week.

05/18/20 (day 36) : 

Finished the WSUS technical documentation.

05/19/20 (day 37) : 

Decided to add Visio graphics to the WSUS technical documentation.

05/21/20 (day 38) : 

Created the PowerPoint presentation version of the WSUS technical documentation.

05/21/20 (day 39) : 

Research on ELK Stack.

05/22/20 (day 40) : 

Research on ELK Stack.

05/26/20 (day 41) : 

Research on ELK Stack.

05/27/20 (day 42) : 

Research on ELK Stack.

Summary of research:
ELK Stack represents multiple components (Elastisearch, Logstash, Kibana, Beats) that work together to aggregate pipelined data
and then visualize the data. Logstash is a server‑side data processing pipeline that ingests data from multiple sources 
simultaneously, transforms it, and then sends it to a "stash" like Elasticsearch. Kibana lets users visualize data with charts
and graphs in Elasticsearch. These components can all be installed on one main server.

The servers that want to participate with ELK stack should install Filebeat. Filebeat is a lightweight shipper for forwarding 
and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify,
collects log events, and forwards them either to Elasticsearch or Logstash for indexing.

Nginx (pronounced "engine-X") can also be installed onto the ELK stack server. Nginx is a reverse proxy. When the user connects to
the Nginx, it proxys the request to Kibana. Kibana pulls the data from Elasticsearch and the user gets the dashboard.

What is a proxy?
In computer networking, a proxy server is a server application or appliance that acts as an intermediary for requests from clients
seeking resources from servers that provide those resources.

The only ports exposed are the Logstash and the Nginx. The other internal ports are localhost.

05/28/20 (day 43) : 

Begin installation of ELK Stack. 

Thomas created a virtual server in https://vcsa.uhtasi.local/. When I initially tried to
get into the VCSA, it would not load after logging in (note this in FireFox). I switched to a Chrome browswer and managed to
get through without issues. When trying to load the console, it would not connect however (Connection Error). I then went to
the Summary tab and then the gear icon. I then went for the Web Console. It opened in a tab in the browser and was working fine.

I logged in using:
user : root
pass : C3nt0st3mpl4t3!

I wanted Thomas to take a snapshot before I fiddled with the networking in nmtui.

I was told to use this IP: 10.100.10.36

06/01/20 (day 44) : 

I got the networking working with nmtui. I began the installation process of the ELK stack using this guide:

https://blog.51sec.org/2019/10/easy-deploy-elk-into-centos-7.html

I got up to the step where I had to vi the /etc/yum.repos.d/elasticsearch.repo

Since I was in the Web Console, it seemed that I could not bring my mouse in to right-click the contents, instead I had to 
type it manually. I did so, but when I ran 

sudo yum install elasticsearch

I was getting an error. I vi'd back into the /etc/yum.repos.d/elasticsearch.repo and noticed the contents were reduced to 

put the contents here once u see them

Instead of trying to type the contents manually, I wished I could have just pasted them in. This is when I went to use PuTTy.
However I could not log in. I was getting Network Error: Connection refused. Thomas asked me if I could ping it. I pinged the
ELK server's IP from the ELK Web Console, and got a response. Obviously this would work. I then pinged the ELK server from the
Command Prompt outside the server, on the actual workstation, and then the ping did not work. This was because the ELK server's 
IP was not on the VPN access list. 

Next time I work I will disconnect and reconnect to the VPN and it should update.

06/02/20 (day 45) :

I began working through PuTTy and had a much easier time with the installation. I installed the netstat command. I also installed
Kibana and configured it so that I created an administrative Kibana user named kibanaadmin. The password for kibanaadmin is 
adminofkibana. I also created

sudo vi /etc/nginx/conf.d/elk.conf

This code configures Nginx to direct your server’s HTTP traffic to the Kibana application, which is listening on localhost:5601. 
Additionally, it configures Nginx to read the htpasswd.users file and require basic authentication. It is a Nginx server block file.

and added 

server_name 10.100.10.36;

which is the public ip for our ELK VM.

The Kibana visualization was live and could be viewed at 10.100.10.36/status

I then went through with the rest of the guide and installed logstash and filebeat.

This was the guide I used:

https://blog.51sec.org/2019/10/easy-deploy-elk-into-centos-7.html

This is the accompanying video:

https://www.youtube.com/watch?v=JeiPj8eSTSY&t=213s

Next time I work I want to see how to get clients integrated with the ELK server.

06/03/20 (day 46) :

We decided not to go with Filebeat on clients. Instead we will try and go with syslog. Initially we wanted to know whether the
format and facilities of syslog would need to be specific for smooth interaction with with the ELK stack.

From Reddit, a user gave me this response:

The facility doesn't matter (it gets recorded as a metadata field), the format is configurable. See here for fluentd, here for logstash, 
but the TL;DR is that it's treated as a log line. If you have RFC3164 logs then that'll work pretty much out of the box with the built in 
patterns. If you have a custom format you can still ship them as log lines, but if you want to parse the fields then you'll need to add a 
specific parser configuration. Both tools are pretty flexible in what they can do, so as long as your format is consistent I don't see any 
issues.

So we went with the old settings TASI had for their Graylog server: Enahnced syslog format and Messages Generated Internally by syslogd as the facility.

I told Thomas to set the port to 5044. We will see if this gets us logs from the clients into our ELK server.

06/04/20 (day 47) :

I could see Kibana receiving logs, about one every five seconds. However it turned out that these logs were filebeat logs from the
ELK server itself. I could also receive logs for certain events happening on the ELK server (like getting disconnected from PuTTY).
The problem was that it seemed I was not receiving logs from outside sources. Thomas tried sending packets from the firewall to my 
ELK server but no information was coming into Kibana. I then noticed that the port I gave him was incorrect, however it did not fix
the problem. I then noticed that my .yaml configuration files actually appeared to be commented out, which was weird.

Thomas said that it looked like I installed the ELK stack but did not configure it properly. 

06/05/20 (day 48) :

I wanted to do a completely new install from elastic.co because that is the official site. Thomas was skeptical however. I also told 
him how I had followed the guide I used exactly, but still could not receive outside data. I told Thomas that the install seemed to be
geared towards working with filebeat, not syslog. Thomas did some quick research on his own and realized that the syslog plug-in was
indeed needed, as I had said earlier. 

When I went to install the syslog plug-in, I noticed the command did not match my file directory structure. I did not have the same file
organization that the elastic.co had. This differing file structure presents difficulties here and potentially down the line.
The command was:

bin/logstash-plugin install logstash-output-syslog

However I did not have bin/logstash-plugin on my server. I looked around in my /etc/logstash folder, but could not find anything that seemed
similar to the bin/logstash-plugin. I decided to revert to before the installation and go with the elastic.co installation next time I work.

Also I noticed that the weekly WSUS reports were being emailed on every Thursday. I am reasoning that I had set the weekly release on a Thursday,
and that is why they are being emailed on that day. I will need to return on a Sunday night to see if it resets this.

06/08/20 (day 49) :

The server was reverted back to its origin state. However when I went into PuTTY the screen was just blank. I then went into the Web Console in
vCenter and noticed that it was frozen on an nmtui page. I told this to Thomas and then he did a "hard reset" of the server. This worked and now
I could use the console and PuTTY fine. I then began the elastic.co installation.

The elastic.co installation separates the ELK components into separate installations, with the order of the components' installation mattering.
I began with the Elasticsearch. 

Once in, instructions depending on operating system are given as options. I went with the Linux option since was what I was working with. 

I was prompted to use the wget command, but I was getting command not found in the terminal. I then tried yum install wget, but I was getting 
Not Found Trying Another Mirror over and over again in the terminal. After some research, it was recommended to use yum clean all. I then used 
yum install wget again and it worked.

I then was prompted to use the shasum command. I did not have this either, but found a way to obtain it online.

I then went to install Kibana and noticed, a bit down the page, that there was the option to use CentOS specific installation instructions.
This could explain why the first download gave me so many problems along the way. I then thought that the doing the Linux install could give
me problems down the line if I do the CentOS install for the other components. I decided to revert completely.

I went through the Elasticsearch install quickly now and then followed up with the Kibana and Logstash. I wanted to test out Kibana before 
incorporating the syslog plugin.

However I was having issues. Despite turning on the Kibana service, and also confirming the port to be localhost:5601 in the .yaml, I was not
able to see the Kibana in the browser. I looked in the .yaml and notice all information seemed to be commented out. However, nowhere in the
instructions says that this is commented out, or mentions anything to do with it. I then disabled the firewall using the process from the old
guide, but I still could not see the Kibana in the browser.

I'm not sure why.

06/09/20 (day 50) :

I began troubleshooting, using commands such as telnet and netstat + grep. Note that telnet actually would not work in this case because 
attempting to telnet to 8.8.8.8 on port 5601 is like me owning 8.8.8.8 and opening 5601 on that address, in other words, it would never work
because I don't control Google.

I then checked SELinux and iptables. Security-Enhanced Linux (SELinux) is a Linux kernel security module that provides a mechanism for supporting
access control security policies, including mandatory access controls (MAC). I checked the SELinux status and it showed that it was enabled and
enforcing. I then freed the security by completely disabling SELinux (this required a reboot which happens through the reboot command). I then began
working with iptables. iptables is a user-space utility program that allows a system administrator to configure the IP packet filter rules of the 
Linux kernel firewall, implemented as different Netfilter modules. I added the 5601 port to iptables.

However even after all of this Kibana was still not being displayed in the browser.

I then uncommented everything in the kibana.yaml, but it did not change anything as I expected. I then went to elasticsearch to see if it was running,
it was not now and using the start command for elasticsearch was failing.

Now everything was backwards, except I want to try something that may work tomorrow. I will reset the server and try again. I think it is something very
small, simple, as it usually is in computer science.

06/10/20 (day 51) :

I tried 127.0.0.1:5601 instead of localhost:5601 because 127.0.0.1 is what was in the /etc/hosts file. However it did not work. I then thought perhaps 
adding localhost to the hosts file could work because some websites do care about the url used to get there. However I was not sure how to do this.

06/12/20 (day 52) :

I tried a new guide:

https://computingforgeeks.com/how-to-install-elk-stack-on-centos-fedora/

However I got stuck at the same step as last time: Kibana not being displayed in the browser. 

I learned that Kibana would start and go active, but then fail after some time. You can observe this behaviour using this command:

journalctl -fu kibana.service

One of the outputs that stood out to me was:

FATAL  Error: [config validation of [elasticsearch].url]: definition for this key is missing

Also you can put this line into the config file, however it did not do anything in this case:

logging.dest: stdout

Thomas then told me that after some research it seemed that there are some problems that arise when doing a Kibana install on root:

https://discuss.elastic.co/t/kibana-service-failed-to-start/191702/9

The reasoning behind this is that many organizations actually create a specific service user to do particular service installs, like
Kibana. Because of this, I created a new, non-sudo CentOS user. Here are the credentials:

user : puppet
pass : citrus878

I will do the install again next time.

06/15/20 (day 53) :

I uninstalled Elasticsearch and Kibana using sudo yum remove elasticsearch and sudo yum remove kibana

I then began the install under the puppet account. I was getting "you need to be root to perform this command" when trying to do the yum install.
I easily gave sudo priviledges by switching back to root and adding puppet to the wheel group. I then went through the install on the puppet account.
When I checked the status of Elasticsearch it was active and running fine. When I checked the status of Kibana I was getting
kibana is not-found (no file or directory). I switched back to root and uninstalled and did the install again but was getting the same output. And
when I went for a systemctl start kibana I would get

Failed to start kibana.service: Unit kibana.service failed to load: No such file or directory.

Also I when I would do the systemctl -enable kibana.service I would get Access denied (on both root and non-root accounts).

Thomas sent me this link which stated that the kibana user must have write permissions to the log file:

https://github.com/elastic/kibana/issues/7041

Therefore I used chmod 777 on the /var/log folder. Note that there was no /var/log/kibana folder like the link had. However after the install
I was getting the same error again.

Thomas then recommended :

journalctl -u kibana.service

It outputed a lot of information, starting from later rather than recent. I found a way online to run the command so that it flipped the output,
so it was most recent first. However, I had trouble interpreting the information. I will look into it more tomorrow.

06/16/20 (day 54) :

We reverted back to the origin snapshot. I configured the networking and then did the installs of Elasticsearch and Kibana on the root account.
I then remade the puppet account and started the Kibana service there. However I got the error where Kibana would not be displayed, despite starting
up fine. I then used the journalctl command to look at the logs. I saw that the Kibana was failing after starting. I looked closer and noticed this 
error:

FATAL Error: [config validation of [elasticsearch].url]: definition for this key is missing

Online I found a thread where this issue was discussed:

https://github.com/elastic/kibana/issues/40335

It looks like the fix might be adding

elasticsearch.hosts: ["http://localhost:5601/"]

to the file 

/etc/kibana/kibana.yml

It is usually always the smallest fixes. Hopefully it works.

06/17/20 (day 55) :

Changing to elasticsearch.hosts worked and now I could finally connect in the browser. However I was getting:

Kibana server is not ready yet

text over a blank screen. I did a

sudo systemctl -l status kibana.service

to see what was going on with Kibana specifically. I got a repeated output of code that contained a message: 
"License information could not be obtained from Elasticsearch due to Service Unavailable" along with the error: 
"Kibana server is not ready yet"

The reason behind this is likely that Kibana is not able to talk with Elasticsearch properly. I need to fix this.

06/18/20 (day 56) :

Was trying to look through logs but could not find anything that stuck out to me. Also note /var/log/elasticsearch was present
but /var/log/kibana was not. Also I noticed that the elasticsearch.yaml was completely commented-out. I uncommented the fields
but I still had the same issue.

06/19/20 (day 57) :

I could not get Elasticsearch to restart. I also tried using the 

sudo systemctl enable --now elasticsearch.service 

as said in the guide but Elasticsearch would not restart. Looking at the logs gave me no info that I could use to help me. I don't
know what else to do. I tried chaning the kibana.yml to

elasticsearch.hosts: ["http://localhost:9200/"]

but it went from Kibana server not yet ready to not even connecting. I decided it would be best to do a sudo yum remove elasticsearch
and kibana.

06/22/20 (day 58) :

The server was reverted and I started from scratch. I got up to the step where I had always been stuck on. However, I ran the 

sudo firewall-cmd --add-port=5601/tcp --permanent
sudo firewall-cmd --reload

and it worked. I think before my problem was that I had 

elasticsearch.hosts: ["http://localhost:5601/"]

when it should have been 

elasticsearch.hosts: ["http://localhost:9200/"]

I also should mentioned I reduced the memory in the /etc/elasticsearch/jvm.options file from 

-Xms1g
-Xmx1g

to

-Xms256m
-Xmx512m

Not sure if it was actually related to our issue.

The main problem here was that Elasticsearch could not communicate with Kibana (since it was not set to port 9200). Now that it is working,
I am moving on to the Logstash configuration.

06/23/20 (day 59) :

I noticed that in the Kibana dashboard you can do a sample data visualization. I chose a web sample. It was nice to play around with it, but 
I had trouble removing it from the dashboard. I ultmately could not remove it.

I continued to fiddle with the Logstash configuration. The Logstash YAML was all commented out, don't know if it is related to our issue.

Also the online guide gives running the logstash-syslog.conf in a way that is not CentOS. Possibly I need to figure out how to run it with
the correct syntax.

06/24/20 (day 60) :

I edited the logstash-syslog.conf so that its input was taking 5044 as its port value. This is the port of TASI's firewall. Thomas then said 
that using systemctl restart logstash should be good enough. I did this, but still could not see logs. Using netstat was recommended, I will do
that next time.

06/25/20 (day 61) :

netstat 5044 was giving a lot of info that was hard for me to interpret. On the other hand, I figured out how to use the bin/logstash command. 
I needed to be within the /usr/share/logstash/bin directory itself to use the command (note the logstash command itself was visible within the
directory). Once there, I could run the command just by typing ./logstash. However the error that would come up would be that it could not find 
the needed YAML configuration files and whatnot. This is because (I believe) I was running the command in the folder where those files where not
present.

I then moved to the /etc/logstash/conf.d folder and of course I could not run the command because I was outside of the logstash command's actual
location. I then learned that I could actually add directories to the $PATH variable. Once the directory is added to this $PATH variable, you can
run the commands within the directory from any directory in the system. So I added /usr/share/logstash/bin to the $PATH variable using the export
command. This worked and I could see the path now added to the default paths in the $PATH variable. It is interesting to learn that the common
commands such as ls and rm are in directories that are in the $PATH variable.

However I could not run the ./logstash command still. After work, I learned from reddit that you do not need the "./"" in front of logstash to run
logstash from outside of its directory, only "logstash" should be typed.

06/26/20 (day 62) :

The new directories that are added to the $PATH variable are not saved after a session ends. Therefore, I had to re-add /usr/share/logstash/bin back
to $PATH. Once I did this, I went to run logstash. It worked, but the launch failed, citing missing config files. I think the issue here may be that
I need to run the command in a certain directory. On elastic.co I learned that the -f tag loads the Logstash config from a specific file or directory,
so perhaps I do not need to be in /etc/logstash/conf.d to start logstash with the logstash-syslog.conf. Instead, maybe I have to be in a different location,
like one listed here:

https://www.elastic.co/guide/en/logstash/7.7/dir-layout.html

06/29/20 (day 63) :

I noticed the firewall did not have the Elasticsearch and Logstash ports open, only the 5601 which belongs to Kibana. I therefore added the 9200, 9300, and
5044 ports. You can view this info by using this command:

firewall-cmd --list-all

Also, I re-added the /usr/share/logstash/bin directory to the $PATH variable. However, I could not run logstash. I am not sure why, because the
path was obviously listed when echo $PATH was ran.

06/30/20 (day 64) :

I once again re-added /usr/share/logstash/bin to $PATH. It actually was working this time. I went to the /etc/logstash/conf.d directory where the logstash-syslog.conf
was located and ran logstash -f logstash-syslog.conf. It looks like it was reading the logstash-syslog.conf, because unlike last time, I was not getting the Error that
the logstash-syslog.conf was not found (I can test this by running the command outside of its directory to see if the error pops up again). 
Instead, I got this output:

[root@templateCentOS7 conf.d]# logstash -f logstash-syslog.conf
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults
Could not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console
[WARN ] 2020-06-24 19:30:58.882 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specified
[INFO ] 2020-06-24 19:30:59.065 [LogStash::Runner] runner - Starting Logstash {"logstash.version"=>"7.8.0", "jruby.version"=>"jruby 9.2.11.1 (2.5.7) 2020-03-25 b1f55b1a40 OpenJDK 64-Bit Server VM 25.252-b09 on 1.8.0_252-b09 +indy +jit [linux-x86_64]"}
[INFO ] 2020-06-24 19:31:16.650 [Converge PipelineAction::Create<main>] Reflections - Reflections took 287 ms to scan 1 urls, producing 21 keys and 41 values
[INFO ] 2020-06-24 19:31:27.004 [[main]-pipeline-manager] elasticsearch - Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[WARN ] 2020-06-24 19:31:28.170 [[main]-pipeline-manager] elasticsearch - Restored connection to ES instance {:url=>"http://localhost:9200/"}
[INFO ] 2020-06-24 19:31:28.922 [[main]-pipeline-manager] elasticsearch - ES Output version determined {:es_version=>7}
[WARN ] 2020-06-24 19:31:28.927 [[main]-pipeline-manager] elasticsearch - Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[INFO ] 2020-06-24 19:31:29.305 [[main]-pipeline-manager] elasticsearch - New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[INFO ] 2020-06-24 19:31:29.487 [Ruby-0-Thread-6: :1] elasticsearch - Using default mapping template
[INFO ] 2020-06-24 19:31:30.153 [Ruby-0-Thread-6: :1] elasticsearch - Attempting to install template {:manage_template=>{"index_patterns"=>"logstash-*", "version"=>60001, "settings"=>{"index.refresh_interval"=>"5s", "number_of_shards"=>1, "index.lifecycle.name"=>"logstash-policy", "index.lifecycle.rollover_alias"=>"logstash"}, "mappings"=>{"dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword", "ignore_above"=>256}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date"}, "@version"=>{"type"=>"keyword"}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}
[INFO ] 2020-06-24 19:31:31.097 [[main]-pipeline-manager] javapipeline - Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>1, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>125, "pipeline.sources"=>["/etc/logstash/conf.d/logstash-syslog.conf"], :thread=>"#<Thread:0x314039a run>"}
[INFO ] 2020-06-24 19:31:36.854 [[main]-pipeline-manager] javapipeline - Pipeline started {"pipeline.id"=>"main"}
[INFO ] 2020-06-24 19:31:36.971 [[main]<tcp] tcp - Starting tcp input listener {:address=>"0.0.0.0:5044", :ssl_enable=>"false"}
[INFO ] 2020-06-24 19:31:37.937 [Agent thread] agent - Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[INFO ] 2020-06-24 19:31:38.043 [[main]<udp] udp - Starting UDP listener {:address=>"0.0.0.0:5044"}
[ERROR] 2020-06-24 19:31:39.171 [[main]<udp] udp - UDP listener died {:exception=>#<Errno::EADDRINUSE: Address already in use - bind - Address already in use>, :backtrace=>["org/jruby/ext/socket/RubyUDPSocket.java:206:in `bind'", "/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-udp-3.3.4/lib/logstash/inputs/udp.rb:116:in `udp_listener'", "/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-udp-3.3.4/lib/logstash/inputs/udp.rb:68:in `run'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:345:in `inputworker'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:336:in `block in start_input'"]}
[INFO ] 2020-06-24 19:31:39.903 [Api Webserver] agent - Successfully started Logstash API endpoint {:port=>9601}
[ERROR] 2020-06-24 19:31:43.441 [[main]<tcp] javapipeline - A plugin had an unrecoverable error. Will restart this plugin.
  Pipeline_id:main
  Plugin: <LogStash::Inputs::Tcp type=>"syslog", port=>5044, id=>"5b5803b5fe889965002c2a1e8e496af4a52aa0b965a43d516bb8ca5f7c8d91d4", enable_metric=>true, codec=><LogStash::Codecs::Line id=>"line_a12eb22e-225b-43fd-b035-b4e8eb191140", enable_metric=>true, charset=>"UTF-8", delimiter=>"\n">, host=>"0.0.0.0", mode=>"server", proxy_protocol=>false, ssl_enable=>false, ssl_verify=>true, ssl_key_passphrase=><password>, tcp_keep_alive=>false, dns_reverse_lookup_enabled=>true>
  Error: Address already in use
  Exception: Java::JavaNet::BindException
  Stack: sun.nio.ch.Net.bind0(Native Method)
sun.nio.ch.Net.bind(sun/nio/ch/Net.java:433)
sun.nio.ch.Net.bind(sun/nio/ch/Net.java:425)
sun.nio.ch.ServerSocketChannelImpl.bind(sun/nio/ch/ServerSocketChannelImpl.java:220)
io.netty.channel.socket.nio.NioServerSocketChannel.doBind(io/netty/channel/socket/nio/NioServerSocketChannel.java:128)
io.netty.channel.AbstractChannel$AbstractUnsafe.bind(io/netty/channel/AbstractChannel.java:558)
io.netty.channel.DefaultChannelPipeline$HeadContext.bind(io/netty/channel/DefaultChannelPipeline.java:1283)
io.netty.channel.AbstractChannelHandlerContext.invokeBind(io/netty/channel/AbstractChannelHandlerContext.java:501)
io.netty.channel.AbstractChannelHandlerContext.bind(io/netty/channel/AbstractChannelHandlerContext.java:486)
io.netty.channel.DefaultChannelPipeline.bind(io/netty/channel/DefaultChannelPipeline.java:989)
io.netty.channel.AbstractChannel.bind(io/netty/channel/AbstractChannel.java:254)
io.netty.bootstrap.AbstractBootstrap$2.run(io/netty/bootstrap/AbstractBootstrap.java:364)
io.netty.util.concurrent.AbstractEventExecutor.safeExecute(io/netty/util/concurrent/AbstractEventExecutor.java:163)
io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(io/netty/util/concurrent/SingleThreadEventExecutor.java:403)
io.netty.channel.nio.NioEventLoop.run(io/netty/channel/nio/NioEventLoop.java:463)
io.netty.util.concurrent.SingleThreadEventExecutor$5.run(io/netty/util/concurrent/SingleThreadEventExecutor.java:858)
io.netty.util.concurrent.FastThreadLocalRunnable.run(io/netty/util/concurrent/FastThreadLocalRunnable.java:30)
java.lang.Thread.run(java/lang/Thread.java:748)
[INFO ] 2020-06-24 19:31:44.188 [[main]<udp] udp - Starting UDP listener {:address=>"0.0.0.0:5044"}
[ERROR] 2020-06-24 19:31:44.201 [[main]<udp] udp - UDP listener died {:exception=>#<Errno::EADDRINUSE: Address already in use - bind - Address already in use>, :backtrace=>["org/jruby/ext/socket/RubyUDPSocket.java:206:in `bind'", "/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-udp-3.3.4/lib/logstash/inputs/udp.rb:116:in `udp_listener'", "/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-udp-3.3.4/lib/logstash/inputs/udp.rb:68:in `run'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:345:in `inputworker'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:336:in `block in start_input'"]}
^C[WARN ] 2020-06-24 19:31:46.261 [SIGINT handler] runner - SIGINT received. Shutting down.
^C[FATAL] 2020-06-24 19:31:47.436 [SIGINT handler] runner - SIGINT received. Terminating immediately..
[ERROR] 2020-06-24 19:31:47.803 [LogStash::Runner] Logstash - org.jruby.exceptions.ThreadKill

Obviously, there are a whole slew of new errors to deal with.

07/01/20 (day 65) :

I wanted to test my theory that moving out of the /etc/logstash/conf.d directory and running logstash -f logstash-syslog.conf would result in failure.
It in fact did. I got this:

[root@templateCentOS7 ~]# logstash -f logstash-syslog.conf
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults
Could not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console
[WARN ] 2020-06-25 17:18:14.356 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specified
[INFO ] 2020-06-25 17:18:14.548 [LogStash::Runner] runner - Starting Logstash {"logstash.version"=>"7.8.0", "jruby.version"=>"jruby 9.2.11.1 (2.5.7) 2020-03-25 b1f55b1a40 OpenJDK 64-Bit Server VM 25.252-b09 on 1.8.0_252-b09 +indy +jit [linux-x86_64]"}
[INFO ] 2020-06-25 17:18:17.075 [Agent thread] configpathloader - No config files found in path {:path=>"/root/logstash-syslog.conf"}
[ERROR] 2020-06-25 17:18:17.097 [Agent thread] sourceloader - No configuration found in the configured sources.
[INFO ] 2020-06-25 17:18:18.458 [LogStash::Runner] runner - Logstash shut down.
[root@templateCentOS7 ~]#

Therefore, I know for certain I need to be within /etc/logstash/conf.d to run the application. Also I want to mention something peculiar that occured
in the /etc/logstash/conf.d directory (note this happened when I right logged in, not after I moved out of the directory and ran the command). 
When I ls within the directory, I get a bunch of jumbled information:

[root@templateCentOS7 conf.d]# ls
[],     jruby 9.2.11.1 (2.5.7) 2020-03-25 b1f55b1a40 OpenJDK 64-Bit Server VM 25.252-b09 on 1.8.0_252-b09 +indy +jit [linux-x86_64]}
7}      logstash-syslog.conf
7.8.0,  {:removed=
9601}
[root@templateCentOS7 conf.d]#

Not sure why, but I can still sudo vi the logstash-syslog.conf file and everything looks normal there.

Anyhow, I want to now troubleshoot the errors displayed when I launch from the etc/logstash/conf.d directory (the ones seen yesterday). Here is
what I will look at specifically:

1.
WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults
Could not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console

To fix this, I will use --path.settings as recommended by the console itself. After researching online, I decided to follow this thread:

https://discuss.elastic.co/t/warning-could-not-find-logstash-yml-which-is-typically-located-in-ls-home-config-or-etc-logstash/131022/5

Here is an example of what they used:

sudo bin/logstash --path.settings /etc/logstash/ --path.data sensor39 -f /home/sayed/logstash/sayed.conf

I want to break their command structure into pieces:

sudo bin/logstash is equivalent to my logstash

--path.settings /etc/logstash/ would be the same as mine

--path.data sensor39 not too sure on this one, or if I need it

-f /home/sayed/logstash/sayed.conf looks like they put the entire path into the command, I guess you can do this

My version of the command would be:

logstash --path.settings /etc/logstash/ -f logstash-syslog.conf

while in the /etc/logstash/conf.d directory.

This worked and I got the errors down to one:

[root@templateCentOS7 conf.d]# logstash --path.settings /etc/logstash -f logstash-syslog.conf
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2020-06-25T18:44:02,891][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2020-06-25T18:44:03,469][FATAL][logstash.runner          ] Logstash could not be started because there is already another instance using the configured data directory.  If you wish to run multiple instances, you must change the "path.data" setting.
[2020-06-25T18:44:03,527][ERROR][org.logstash.Logstash    ] java.lang.IllegalStateException: Logstash stopped processing because of an error: (SystemExit) exit
[root@templateCentOS7 conf.d]#

So now onto the next error!

2.
[ERROR] 2020-06-24 19:31:39.171 [[main]<udp] udp - UDP listener died {:exception=>#<Errno::EADDRINUSE: Address already in use - bind - Address already in use>, :backtrace=>["org/jruby/ext/socket/RubyUDPSocket.java:206:in `bind'", "/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-udp-3.3.4/lib/logstash/inputs/udp.rb:116:in `udp_listener'", "/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-udp-3.3.4/lib/logstash/inputs/udp.rb:68:in `run'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:345:in `inputworker'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:336:in `block in start_input'"]}
[INFO ] 2020-06-24 19:31:39.903 [Api Webserver] agent - Successfully started Logstash API endpoint {:port=>9601}
[ERROR] 2020-06-24 19:31:43.441 [[main]<tcp] javapipeline - A plugin had an unrecoverable error. Will restart this plugin.
  Pipeline_id:main
  Plugin: <LogStash::Inputs::Tcp type=>"syslog", port=>5044, id=>"5b5803b5fe889965002c2a1e8e496af4a52aa0b965a43d516bb8ca5f7c8d91d4", enable_metric=>true, codec=><LogStash::Codecs::Line id=>"line_a12eb22e-225b-43fd-b035-b4e8eb191140", enable_metric=>true, charset=>"UTF-8", delimiter=>"\n">, host=>"0.0.0.0", mode=>"server", proxy_protocol=>false, ssl_enable=>false, ssl_verify=>true, ssl_key_passphrase=><password>, tcp_keep_alive=>false, dns_reverse_lookup_enabled=>true>
  Error: Address already in use
  Exception: Java::JavaNet::BindException
  Stack: sun.nio.ch.Net.bind0(Native Method)

I see the error here is <Errno::EADDRINUSE: Address already in use - bind - Address already in use>
My instinct tells me that this is because I already have a running instance of either Elasticsearch or Logstash, and therefore the address used by those processes
is being taken up. To fix this, I will stop those services and then run the command.

To do this I will work with these series of commands:

service logstash stop
service logstash start
service logstash status -l

systemctl stop logstash.service
systemctl start logstash.service
systemctl status logstash.service -l

Stopping logstash and the logstash service seemed to work. Doing the status gave that the process had been killed and that it is now failed/dead. Also I want to mention
that prior to this (when Logstash was coming back as Active) it said this active instance of Logstash had been running since June 23 (06/23).

I re-ran the logstash --path.settings /etc/logstash/ -f logstash-syslog.conf and there were no errors at all. I got to the line saying something like Logstash successfully 
started on endpoint 9200 or something. However it was just stuck on that line, without going to the next line in the terminal. I just waited it out, until PuTTY actually 
timedout. When I went reloaded PuTTY, I did a systemctl status logstash and got back that Logstash was killed/inactive. I then ran the 
logstash --path.settings /etc/logstash/ -f logstash-syslog.conf again but it was back to the old error of saying the Logstash instance is occupied by another instance, which
was weird because I was getting that Logstash was dead. I did the stop logstash command again just in case but was getting the same error. Not sure why.

[2020-06-25T19:40:20,895][FATAL][logstash.runner          ] Logstash could not be started because there is already another instance using the configured data directory.  If you wish to run multiple instances, you must change the "path.data" setting.

07/06/20 (day 66) :

Stopped logstash and elasticsearch this time. Launched service again, got the same error that another Logstash instance was running. 

I looked into the "If you wish to run multiple instances, you must change the "path.data" setting." part of the error. 

I followed this guide and it worked:

https://www.programmersought.com/article/4108631457/

I was then getting a different error, "a plug-in had an unrecoverable error". I did a systemctl status logstash and saw it was not active. I then
started the service, but then got the [Address already in use] error. I stopped logstash again, and got a new error:

[ERROR][io.netty.util.concurrent.DefaultPromise.rejectedExecution][main][5b5803b5fe889965002c2a1e8e496af4a52aa0b965a43d516bb8ca5f7c8d91d4] Failed to submit a listener notification task. Event loop shut down?
java.util.concurrent.RejectedExecutionException: event executor terminated

This could be farfetched, but perhaps the error could be related to the fact that Logstash launched successfully last time I worked, and it never shut off.
Therefore, when I am trying to run it now, the launch failure is related to this.

07/07/20 (day 67) :

Was still experiecing the Address in use Error. Used WinSCP to transfer the logstash.yml from the server to my desktop to send to Thomas. Thomas could not identify
the error and said he would have to go onto the server itself and poke around when he had time. I went onto Kibana -> Index Management and saw a 
logstash.2020.06.18 listed. So it seems that there was a successful connection made between Logstash and Kibana some time ago. I went to create an Index Pattern but
could not find any Elasticsearch indices when using the search query "logstash". So something must be wrong. Overall, the entire situation felt fishy. I think it is
linked to the launch being successful, but then timing out, that might be the source of the glitch. I think what might be best would be to revert back to the snapshot
before I deleted the .lock file. Thomas said the second to last snapshot was taken on 6/25. Notably, for technical consequences, this state was before I added
ports 9200, 9300, and 5044 to the firewall (also need to reload).

07/08/20 (day 68) :

I reverted back to 6/25 snapshot. I added the new firewall ports. I then launched Logstash and got this:

[root@templateCentOS7 conf.d]# logstash --path.settings /etc/logstash/ -f logstash-syslog.conf
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2020-06-19T19:25:44,228][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2020-06-19T19:25:45,127][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.8.0", "jruby.version"=>"jruby 9.2.11.1 (2.5.7) 2020-03-25 b1f55b1a40 OpenJDK 64-Bit Server VM 25.252-b09 on 1.8.0_252-b09 +indy +jit [linux-x86_64]"}
[2020-06-19T19:25:58,279][INFO ][org.reflections.Reflections] Reflections took 178 ms to scan 1 urls, producing 21 keys and 41 values
[2020-06-19T19:26:06,682][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2020-06-19T19:26:08,089][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2020-06-19T19:26:08,761][INFO ][logstash.outputs.elasticsearch][main] ES Output version determined {:es_version=>7}
[2020-06-19T19:26:08,894][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2020-06-19T19:26:10,023][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2020-06-19T19:26:10,452][INFO ][logstash.outputs.elasticsearch][main] Using default mapping template
[2020-06-19T19:26:11,290][INFO ][logstash.outputs.elasticsearch][main] Attempting to install template {:manage_template=>{"index_patterns"=>"logstash-*", "version"=>60001, "settings"=>{"index.refresh_interval"=>"5s", "number_of_shards"=>1, "index.lifecycle.name"=>"logstash-policy", "index.lifecycle.rollover_alias"=>"logstash"}, "mappings"=>{"dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword", "ignore_above"=>256}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date"}, "@version"=>{"type"=>"keyword"}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}
[2020-06-19T19:26:12,168][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>1, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>125, "pipeline.sources"=>["/etc/logstash/conf.d/logstash-syslog.conf"], :thread=>"#<Thread:0x5d437e20 run>"}
[2020-06-19T19:26:16,686][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2020-06-19T19:26:16,768][INFO ][logstash.inputs.tcp      ][main][5b5803b5fe889965002c2a1e8e496af4a52aa0b965a43d516bb8ca5f7c8d91d4] Starting tcp input listener {:address=>"0.0.0.0:5044", :ssl_enable=>"false"}
[2020-06-19T19:26:17,392][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2020-06-19T19:26:17,524][INFO ][logstash.inputs.udp      ][main][ffe4a9808fa9947c4f540a8fadf2a5ffc2d2fffde75c49594848e2261bb4dc2d] Starting UDP listener {:address=>"0.0.0.0:5044"}
[2020-06-19T19:26:17,915][INFO ][logstash.inputs.udp      ][main][ffe4a9808fa9947c4f540a8fadf2a5ffc2d2fffde75c49594848e2261bb4dc2d] UDP listener started {:address=>"0.0.0.0:5044", :receive_buffer_bytes=>"106496", :queue_size=>"2000"}
[2020-06-19T19:26:18,616][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}

It was stuck here so I did a ^C before it timed-out and got:

^C[2020-06-19T19:38:31,897][WARN ][logstash.runner          ] SIGINT received. Shutting down.
^C[2020-06-19T19:38:32,298][FATAL][logstash.runner          ] SIGINT received. Terminating immediately..
[2020-06-19T19:38:32,593][ERROR][org.logstash.Logstash    ] org.jruby.exceptions.ThreadKill

Doing a systemctl status logstash gave that Logstash was inactive and killed.

07/09/20 (day 69) :

I ran the command and still got up to the same step, with it just hanging there. I went to check the Kibana dashboard but could not find anything new or Logstash
related in the indices. Not sure what else to do...

07/10/20 (day 70) :

Installed Filebeat on the server. Eventually realized that Filebeat is meant to be installed on other servers that will be sending data to the Logstash server.

07/13/20 (day 71) :

I followed these instructions on the Kibana dashboard and managed to get Filebeat set-up:

http://10.100.10.36:5601/app/kibana#/home/tutorial/systemLogs

But I was only getting the templateCentOS logs, not logs comming in from the NSA 2650 SonicOS firewall. Also I accidently removed the OS column in the syslog logs display,
I could not figure out how to re-add it.

I also managed to remove the sample data, with instructions towards the end of this guide:

https://www.elastic.co/guide/en/kibana/current/tutorial-sample-data.html

07/14/20 (day 72) :

I looked into the command tcpdump. This command allows the user to display TCP/IP and other packets being transmitted or received over a network to which the computer is attached.
tcpdump -D shows the interfaces that are available/coming in. I used this command:

[root@templateCentOS7 ~]# tcpdump -D
1.nflog (Linux netfilter log (NFLOG) interface)
2.nfqueue (Linux netfilter queue (NFQUEUE) interface)
3.eno16777984
4.any (Pseudo-device that captures on all interfaces)
5.lo [Loopback]

Thomas told me eno16777984 was the firewall that was sending the syslog logs. I then ran this command:

tcpdump -i eno16777984 src 10.100.10.2

and got a ton of data being spat out on through the terminal. This was expected, since the firewall was sending a lot of info constantly.

Thomas then recommended taking out the grok{} portion of the logstash-syslog.yml, since it actually may just be overcomplicating things. 
Removing it should not effect anything too important because it is just the filter. So I did this.

Before running logstash --path.settings /etc/logstash/ -f logstash-syslog.conf I told Thomas to take a snapshot so that if I got a timeout I
wouldn't run in to the same issue as last time (no clear fix, having the glitch). The snapshot was called "no filter logstash test".

So I ran the command and it reached:

[2020-06-25T18:46:24,658][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}

as usual. I then thought that I should free up the 9600 port in the firewall as well, since I had not opened that. Thomas then told me to 
open 5044/udp as well, since syslog sends through udp not tcp usually. I did this.

Also for some reason syslog was configured to be sending through 5601, not 5044. Thomas changed it back to 5044 on his end.

I then re-ran the command, and once it reached the 

[2020-06-25T18:46:24,658][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}

it actually began spitting out firewall/syslog logs! Next step is to get it into Kibana and begin parsing the messages.

07/15/20 (day 73) :

To get the syslog logs out of the terminal and into Kibana, basically I need an Elasticsearch instance that Logstash 
can send to and Kibana to read from ES. I wanted to change the port in the elasticsearch.yml from 9200 to 9600, because
this is what the Logstash says it successfully is launching from.

I went into the elasticsearch.yml in /etc/elasticsearch and found that the http.port: 9200 was actually commented out.
I uncommented it out and set it to 9600. I also then did a sudo systemctl restart elasticsearch to cement the changes.
However I was getting an error message:

Job for elasticsearch.service failed because the control process exited with error code.

I guess it is related to me changing the .yml

I went back into the .yml and to the Network block where http.port was located, I thought perhaps I would need to 
uncomment the above network.hosts line as well. I did this, but when I did another restart I got the same error.
I recommented the lines and did a restart, no error now, and ES was coming back as active and running.

07/16/20 (day 74) :

Further research/troubleshooting on ELK Stack.

07/16/20 (day 74) :

I  learned that I am actually supposed to wait-out the terminal output, it means that all of those syslogs are being indexed into Elasticsearch. 
So I guess I will leave the terminal running until the output is finished. Thomas also said that the logs are being generated pretty much every second.
There might be a way to view the logs in Kibana while it is still being indexed in ES.

07/17/20 (day 75) :

Quick day. Further troubleshooting.

07/20/20 (day 76) :

Went into Dev Tools and tried to see if I could see any incoming data. I couldn't. I then let the Logstash run on for too long and it timed out, which gave
me the familiar glitch with no clear fix. I could not fix it despite removing the .lock file (the file keeps regenerating). I will need to revert to the
last snapshot.

This is the new video that I have been following:
https://www.youtube.com/watch?v=imrKm6dV3NQ

07/21/20 (day 77) :

Quick day. Further troubleshooting. Reverted to 7/14 snapshot.

07/22/20 (day 78) :

Decided to revisit the WSUS server to see how things were going. The workstations were really clogged up (5000+ updates on the servers). 
Generating the WSUS-made report resulted in time-out. I decided to run the Adam Clean-Up.ps1 again. Remember to run it from administrator
in ISE (do this by right-clicking on the regular Powershell icon and selecting the Run as Administrator option). I used the -FirstRun tag,
but noticed the -MonthlyRun, which could be useful.

I then went back to the ELK server to fiddle with the elasticsearch.yml. I uncommented the 9200 port again and did a systemctl restart elasticsearch
I then did a systemctl status elasticsearch and it actually came back as active and running. I ran the logstash command but once it finished I was not
getting syslogs, and I remembered I forgot to open the necessary ports in the firewall. I did this and then did a snapshot called "firewall done".

However I still could not get anything to come up in Kibana.

07/23/20 (day 79) :

The first fix I thought could change something was putting the filter{} block of code back into the logstash-syslog.conf file. I did this but was
getting a grokparsefailure message along with each syslog coming through the terminal. Also going to Dev Tools and doing the run gave a stack timeout
type of error instead of nothing unlike last time.

Played around with disabling SELinux and the firewall. I edited the SELinux's config file in vi to permissive and did a reboot. This didn't really
change anything however. Though when I did run logstash again I was getting an error that I was not even connecting to Elasticsearch. I went
back to /etc/elasticsearch/elasticsearch.yml and re-commented the http.port: 9200 line and did a restart on elasticsearch. This fixed the issue
and I was getting syslog logs again.

07/24/20 + 07/26/20 (day 80 + Sunday outside hours) :

I did research online, learning that incoming logs are given an auto-created index. This reddit thread that I created helped a lot:

https://www.reddit.com/r/elkstack/comments/hwvqsd/need_help_viewing_incoming_syslogs_in_kibana/

Basically, read my conversation with daqqad in the thread and you will learn what I did. So now I am able to actually view the syslog logs in
the Discover tab. So this entire time, the connection actually was there, I just did not have enough experience with Kibana to see the connection.
Well anyways, next time I will work with visualizations. Also I want to learn what the syslogs logs actually are saying, because as of now, I do
not know what they are trying to communicate.

I also logged into the WSUS server and set the reports to be emailed weekly. This will have the effect of the reports to be emailed on Sundays, because
it was a Sunday when I did this. Once again, this is only my theory, I will check on this to see if it is in fact doing this. Also, I scheduled a clean
script to auto-run on Sundays at 11:59:59.

*
*
*
*
*
*
*
*













a